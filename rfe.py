# -*- coding: utf-8 -*-
"""O1-RFE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QrbikhXWGZ1x2GLOF1G_2MytU31aPnmB
"""



# =========================
# RFE Citation Map Builder (OpenAlex)
# Works with a list of your DOIs
# Outputs:
#  - your_works.csv
#  - citing_works.csv
#  - citations_by_country_top18.csv + PNG bar chart
#  - citations_by_institution_top18.csv + PNG bar chart
#  - top_citing_authors.csv
#  - citation_map_edges_institution_to_yourwork.csv
#  - citation_map.graphml (for Gephi) + PNG preview (simple)
# =========================

!pip -q install pandas requests tqdm networkx matplotlib pycountry

import re
import time
import math
import json
import pandas as pd
import requests
from tqdm import tqdm
import networkx as nx
import matplotlib.pyplot as plt
import pycountry

# -------------------------
# INPUTS
# -------------------------
YOUR_DOIS = [
    "10.18653/v1/2024.acl-long.739",
    "10.48550/arxiv.2406.17169",
    "10.48550/arxiv.2407.14790",
    "10.1109/TNSM.2023.3287393",
    "10.48550/arxiv.2309.04635",
    "10.48550/arxiv.2305.12096",
    "10.1109/ACCESS.2023.3233983",
    "10.1109/ACCESS.2022.3163305",
    "10.1109/ACCESS.2022.3163023",
    "10.3390/math10152566",
    "10.1109/ACCESS.2021.3117848",
    "10.22541/au.166723035.57811042/v1"
]

# Put your email here (OpenAlex "polite pool" recommended)
MAILTO_EMAIL = "nppatel7@asu.edu"

# How many top entities to export/plot for the RFE checklist
TOP_K = 18

# -------------------------
# OpenAlex helpers
# -------------------------
OPENALEX_BASE = "https://api.openalex.org"

session = requests.Session()
session.headers.update({"User-Agent": "RFE-Citation-Map/1.0 (mailto:%s)" % MAILTO_EMAIL})

def normalize_doi(d: str) -> str:
    d = d.strip()
    d = re.sub(r"^https?://(dx\.)?doi\.org/", "", d, flags=re.IGNORECASE)
    return d.lower()

def doi_to_openalex_doi_url(d: str) -> str:
    # OpenAlex canonical DOI value is typically stored as https://doi.org/<doi>
    return f"https://doi.org/{normalize_doi(d)}"

def oa_get(path: str, params: dict | None = None, max_retries: int = 5):
    """
    Simple robust GET with retries + rate limit handling.
    """
    if params is None:
        params = {}
    # add mailto to any request (recommended by OpenAlex docs)
    params = dict(params)
    params["mailto"] = MAILTO_EMAIL

    url = f"{OPENALEX_BASE}{path}"
    backoff = 1.5
    for attempt in range(max_retries):
        r = session.get(url, params=params, timeout=60)
        if r.status_code == 200:
            return r.json()
        if r.status_code in (429, 500, 502, 503, 504):
            time.sleep(backoff)
            backoff *= 1.8
            continue
        # Other errors: raise
        raise RuntimeError(f"OpenAlex error {r.status_code}: {r.text[:500]}")
    raise RuntimeError(f"Failed after retries: {url}")

def oa_list_all_works(filter_str: str, select: str, per_page: int = 200, max_items: int | None = None):
    """
    Cursor-pagination for /works.
    """
    results = []
    cursor = "*"
    with tqdm(total=None, desc="Fetching works") as pbar:
        while True:
            data = oa_get(
                "/works",
                params={
                    "filter": filter_str,
                    "select": select,
                    "per-page": per_page,
                    "cursor": cursor,
                },
            )
            batch = data.get("results", [])
            results.extend(batch)
            pbar.update(len(batch))

            if max_items is not None and len(results) >= max_items:
                return results[:max_items]

            cursor = data.get("meta", {}).get("next_cursor")
            if not cursor or len(batch) == 0:
                break
    return results

def safe_country_name(cc: str) -> str:
    if not cc or cc == "unknown":
        return "Unknown"
    try:
        return pycountry.countries.get(alpha_2=cc.upper()).name
    except Exception:
        return cc.upper()

# -------------------------
# Step 1: Fetch your works via DOI (bulk)
# Uses OpenAlex OR filter syntax with '|'
# -------------------------
dois_norm = [normalize_doi(d) for d in YOUR_DOIS]
dois_oa = [doi_to_openalex_doi_url(d) for d in dois_norm]

# Bulk DOI lookup: filter=doi:<doi1>|<doi2>|...
filter_yours = "doi:" + "|".join(dois_oa)

your_works = oa_list_all_works(
    filter_str=filter_yours,
    select="id,doi,display_name,publication_year,cited_by_count,primary_location,ids",
    per_page=200,
)

if len(your_works) == 0:
    raise RuntimeError("No works found for your DOIs in OpenAlex. Double-check DOI formatting.")

# Map: OpenAlex WorkID -> your DOI
your_rows = []
your_work_ids = []
for w in your_works:
    wid = w.get("id", "")
    your_work_ids.append(wid.replace("https://openalex.org/", ""))
    your_rows.append({
        "openalex_id": w.get("id"),
        "openalex_work_id": wid.replace("https://openalex.org/", ""),
        "doi": w.get("doi"),
        "title": w.get("display_name"),
        "year": w.get("publication_year"),
        "openalex_cited_by_count": w.get("cited_by_count"),
    })

df_yours = pd.DataFrame(your_rows).sort_values(by="openalex_cited_by_count", ascending=False)
df_yours.to_csv("your_works.csv", index=False)

print("Found your works in OpenAlex:", len(df_yours))
display(df_yours)

# -------------------------
# Step 2: Fetch ALL citing works that cite ANY of your works
# filter=cites:W1|W2|...
# This avoids double-counting a citing work that cites multiple of yours.
# -------------------------
filter_citing = "cites:" + "|".join(your_work_ids)

citing_works = oa_list_all_works(
    filter_str=filter_citing,
    select="id,doi,display_name,publication_year,primary_location,authorships,referenced_works,ids",
    per_page=200,
)

print("Total unique citing works (OpenAlex):", len(citing_works))

# -------------------------
# Step 3: For each citing work, compute which of YOUR works it cites
# -------------------------
your_work_id_set = set("https://openalex.org/" + wid for wid in your_work_ids)

def extract_venue_name(w):
    # Try host_venue first, else primary_location->source
    hv = w.get("host_venue") or {}
    if hv.get("display_name"):
        return hv.get("display_name")
    pl = (w.get("primary_location") or {}).get("source") or {}
    return pl.get("display_name")

citing_rows = []
for w in citing_works:
    ref = set(w.get("referenced_works") or [])
    cited_yours = sorted(list(ref.intersection(your_work_id_set)))
    cited_yours_short = [x.replace("https://openalex.org/", "") for x in cited_yours]

    citing_rows.append({
        "openalex_id": w.get("id"),
        "doi": w.get("doi"),
        "title": w.get("display_name"),
        "year": w.get("publication_year"),
        "venue": extract_venue_name(w),
        "cites_your_openalex_work_ids": ";".join(cited_yours_short),
        "num_your_papers_cited_in_this_work": len(cited_yours_short),
    })

df_citing = pd.DataFrame(citing_rows).sort_values(by=["year", "title"], ascending=[False, True])
df_citing.to_csv("citing_works.csv", index=False)

# Summary stats
total_incoming_citations = len(df_citing)  # unique citing works (OpenAlex definition here)
print("\n=== SUMMARY (OpenAlex) ===")
print("Your works found:", len(df_yours))
print("Unique citing works:", total_incoming_citations)

# -------------------------
# Step 4: Aggregate citations by COUNTRY and INSTITUTION (RFE top-18 lists)
# We count a citing work once for each country/institution represented in its authorships.
# This matches OpenAlex group-by semantics on authorships institutions fields.
# -------------------------
country_counts = {}
inst_counts = {}  # key: institution_id -> dict(name, country_code, count)
author_counts = {} # key: author_id -> dict(name, count)

for w in citing_works:
    authorships = w.get("authorships") or []

    seen_countries = set()
    seen_insts = set()
    seen_authors = set()

    for a in authorships:
        # Authors
        au = (a.get("author") or {})
        au_id = au.get("id")
        au_name = au.get("display_name")
        if au_id and au_id not in seen_authors:
            seen_authors.add(au_id)
            if au_id not in author_counts:
                author_counts[au_id] = {"author_name": au_name, "count": 0}
            author_counts[au_id]["count"] += 1

        # Institutions
        for inst in (a.get("institutions") or []):
            iid = inst.get("id")
            iname = inst.get("display_name")
            icc = inst.get("country_code") or "unknown"

            if iid and iid not in seen_insts:
                seen_insts.add(iid)
                if iid not in inst_counts:
                    inst_counts[iid] = {"institution_name": iname, "country_code": icc, "count": 0}
                inst_counts[iid]["count"] += 1

            if icc and icc not in seen_countries:
                seen_countries.add(icc)
                country_counts[icc] = country_counts.get(icc, 0) + 1

df_country = (
    pd.DataFrame([{"country_code": k, "country": safe_country_name(k), "citing_works": v} for k, v in country_counts.items()])
    .sort_values("citing_works", ascending=False)
)
df_inst = (
    pd.DataFrame([{"institution_id": k, **v} for k, v in inst_counts.items()])
    .assign(country=lambda d: d["country_code"].map(safe_country_name))
    .sort_values("count", ascending=False)
    .rename(columns={"count": "citing_works"})
)
df_auth = (
    pd.DataFrame([{"author_id": k, **v} for k, v in author_counts.items()])
    .sort_values("count", ascending=False)
    .rename(columns={"count": "citing_works"})
)

df_country_top = df_country.head(TOP_K)
df_inst_top = df_inst.head(TOP_K)
df_auth_top = df_auth.head(50)

df_country_top.to_csv("citations_by_country_top18.csv", index=False)
df_inst_top.to_csv("citations_by_institution_top18.csv", index=False)
df_auth_top.to_csv("top_citing_authors.csv", index=False)

display(df_country_top)
display(df_inst_top)
display(df_auth_top.head(15))

# -------------------------
# Step 5: Plot charts (PNG)
# -------------------------
def plot_bar(df, x_col, y_col, title, out_png, rotate=60):
    plt.figure(figsize=(12, 6))
    plt.bar(df[x_col], df[y_col])
    plt.title(title)
    plt.xticks(rotation=rotate, ha="right")
    plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.show()

plot_bar(df_country_top, "country", "citing_works", f"Citations (unique citing works) by Country — Top {TOP_K}", "citations_by_country_top18.png")
plot_bar(df_inst_top, "institution_name", "citing_works", f"Citations (unique citing works) by Institution — Top {TOP_K}", "citations_by_institution_top18.png")

# -------------------------
# Step 6: Build a "Citation Map" (Institution -> Your Paper) edge list
# Edge weight = number of citing works from that institution that cite that specific your paper.
# -------------------------
# Build quick lookup for your papers
your_id_to_title = {row["openalex_work_id"]: row["title"] for _, row in df_yours.iterrows()}

edge_counts = {}  # (inst_id, your_work_id) -> count

for w in citing_works:
    ref = set(w.get("referenced_works") or [])
    cited_yours = [x.replace("https://openalex.org/", "") for x in ref.intersection(your_work_id_set)]

    if not cited_yours:
        continue

    # get institutions in this citing work
    insts_in_work = set()
    for a in (w.get("authorships") or []):
        for inst in (a.get("institutions") or []):
            iid = inst.get("id")
            if iid:
                insts_in_work.add(iid)

    for iid in insts_in_work:
        for yid in cited_yours:
            key = (iid, yid)
            edge_counts[key] = edge_counts.get(key, 0) + 1

edges = []
for (iid, yid), c in edge_counts.items():
    inst_meta = inst_counts.get(iid, {})
    edges.append({
        "institution_id": iid,
        "institution_name": inst_meta.get("institution_name"),
        "institution_country": safe_country_name(inst_meta.get("country_code")),
        "your_openalex_work_id": yid,
        "your_paper_title": your_id_to_title.get(yid),
        "edge_weight_citing_works": c
    })

df_edges = pd.DataFrame(edges).sort_values("edge_weight_citing_works", ascending=False)
df_edges.to_csv("citation_map_edges_institution_to_yourwork.csv", index=False)
display(df_edges.head(25))

# -------------------------
# Step 7: Export graph (GraphML for Gephi) + simple preview
# -------------------------
G = nx.DiGraph()

# Add your-paper nodes
for yid, title in your_id_to_title.items():
    G.add_node(f"Y:{yid}", node_type="your_paper", label=title)

# Add institution nodes and edges
for _, r in df_edges.iterrows():
    inst_node = f"I:{r['institution_id']}"
    if inst_node not in G:
        G.add_node(
            inst_node,
            node_type="institution",
            label=r["institution_name"],
            country=r["institution_country"],
        )
    G.add_edge(inst_node, f"Y:{r['your_openalex_work_id']}", weight=int(r["edge_weight_citing_works"]))

nx.write_graphml(G, "citation_map.graphml")
print("Saved GraphML:", "citation_map.graphml")

# Simple preview plot (not a fancy layout; for real visuals use Gephi)
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G, k=0.45, seed=42)
nx.draw_networkx_nodes(G, pos, node_size=80)
nx.draw_networkx_edges(G, pos, alpha=0.25, arrows=False)
plt.title("Citation Map Preview (Institution -> Your Paper)")
plt.axis("off")
plt.tight_layout()
plt.savefig("citation_map_preview.png", dpi=200)
plt.show()

print("\nDone. Files saved in Colab workspace:")
for f in [
    "your_works.csv",
    "citing_works.csv",
    "citations_by_country_top18.csv",
    "citations_by_institution_top18.csv",
    "top_citing_authors.csv",
    "citations_by_country_top18.png",
    "citations_by_institution_top18.png",
    "citation_map_edges_institution_to_yourwork.csv",
    "citation_map.graphml",
    "citation_map_preview.png",
]:
    print(" -", f)











!pip -q install pandas requests



import os, time, json, re
import requests
import pandas as pd

AUTHOR_ID = "Mwd7y2QAAAAJ"   # from https://scholar.google.com/citations?user=...
OUTDIR = "out"
os.makedirs(OUTDIR, exist_ok=True)

SERPAPI_KEY = os.environ.get("SERPAPI_KEY")
if not SERPAPI_KEY:
    raise RuntimeError("Missing SERPAPI_KEY env var.")

SERPAPI_ENDPOINT = "https://serpapi.com/search.json"

def serpapi_get(params: dict, sleep_s: float = 1.0) -> dict:
    """Small wrapper with backoff-friendly pacing."""
    params = dict(params)
    params["api_key"] = SERPAPI_KEY
    r = requests.get(SERPAPI_ENDPOINT, params=params, timeout=60)
    r.raise_for_status()
    time.sleep(sleep_s)
    return r.json()

def venue_guess(publication: str) -> str:
    # Scholar "publication" strings look like: "Journal Name 12(3), 1-10, 2022"
    if not publication:
        return ""
    first = publication.split(",")[0].strip()
    # remove trailing year if it sneaks into first chunk
    first = re.sub(r"\b(19|20)\d{2}\b$", "", first).strip()
    return first

# -------- 1) Fetch ALL articles (pagination) --------
all_articles = []
start = 0
num = 100  # max per docs :contentReference[oaicite:4]{index=4}

author_payload_first = None

while True:
    payload = serpapi_get({
        "engine": "google_scholar_author",
        "author_id": AUTHOR_ID,
        "hl": "en",
        "start": start,
        "num": num
    })
    if author_payload_first is None:
        author_payload_first = payload

    articles = payload.get("articles", []) or []
    if not articles:
        break

    all_articles.extend(articles)
    if len(articles) < num:
        break
    start += num

author_payload = author_payload_first or {}
author_info = author_payload.get("author", {}) or {}
cited_by = author_payload.get("cited_by", {}) or {}
cited_by_table = cited_by.get("table", []) or []
cited_by_graph = cited_by.get("graph", []) or []

profile_name = author_info.get("name", "")
profile_affiliation = author_info.get("affiliations", "")

# total citations in Scholar UI (from cited_by.table[0].citations.all) :contentReference[oaicite:5]{index=5}
total_citations = None
try:
    total_citations = int(cited_by_table[0]["citations"]["all"])
except Exception:
    # fallback: sum graph (strings)
    total_citations = sum(int(x.get("citations", "0")) for x in cited_by_graph if str(x.get("citations","0")).isdigit())

total_papers = len(all_articles)

# -------- 2) CSV1: citations by year --------
rows1 = []
if cited_by_graph:
    for item in cited_by_graph:
        y = item.get("year")
        c = item.get("citations", "0")
        rows1.append({
            "total_papers": total_papers,
            "total_citations": total_citations,
            "year": y,
            "citations_in_year": int(c) if str(c).isdigit() else c,
            "profile_name": profile_name,
            "profile_affiliation": profile_affiliation,
        })
else:
    rows1.append({
        "total_papers": total_papers,
        "total_citations": total_citations,
        "year": "",
        "citations_in_year": "",
        "profile_name": profile_name,
        "profile_affiliation": profile_affiliation,
    })

df1 = pd.DataFrame(rows1)
csv1_path = os.path.join(OUTDIR, "profile_citations_by_year.csv")
df1.to_csv(csv1_path, index=False)

# -------- 3) CSV2: per-paper details (optionally deeper via view_citation) --------
# NOTE: view_citation is supported via view_op + citation_id :contentReference[oaicite:6]{index=6}

FETCH_VIEW_CITATION = True   # set False if you only want list_works-level fields
MAX_PUBS = None              # set to e.g. 20 while testing

rows2 = []
for idx, a in enumerate(all_articles):
    if MAX_PUBS is not None and idx >= MAX_PUBS:
        break

    title = a.get("title", "")
    publication = a.get("publication", "")
    year = a.get("year", "")
    authors = a.get("authors", "")
    link = a.get("link", "")
    citation_id = a.get("citation_id", "")

    cited_by_obj = a.get("cited_by", {}) or {}
    citations = cited_by_obj.get("value", 0)
    citation_details_link = cited_by_obj.get("link", "")

    # Optional: fetch the paper's Scholar "view_citation" payload to get richer fields + per-year citations table
    citations_by_year = []
    journal = ""
    publication_date = ""
    description = ""
    resources = []

    if FETCH_VIEW_CITATION and citation_id:
        cit = serpapi_get({
            "engine": "google_scholar_author",
            "author_id": AUTHOR_ID,
            "view_op": "view_citation",
            "citation_id": citation_id,
            "hl": "en"
        })

        citation = (cit.get("citation") or {})
        # many are available per docs :contentReference[oaicite:7]{index=7}
        journal = citation.get("journal", "") or citation.get("publication", "")
        publication_date = citation.get("publication_date", "")
        description = citation.get("description", "")
        resources = citation.get("resources", []) or []

        total_cits = citation.get("total_citations", {}) or {}
        table = total_cits.get("table", []) or []
        for t in table:
            y = t.get("year")
            c = t.get("citations")
            if y is not None and c is not None:
                citations_by_year.append({"year": y, "citations": c})

        # sometimes this has a better cited-by link too
        cb = (total_cits.get("cited_by") or {})
        if cb.get("link"):
            citation_details_link = cb["link"]

    rows2.append({
        "paper_title": title,
        "citations": citations,
        "citation_details": citation_details_link,  # "Cited by" link
        "year": year,
        "venue": venue_guess(publication),
        "publication_raw": publication,
        "country": "",  # Scholar UI doesn't reliably provide this
        "link": link,   # Scholar citation page link for the paper (or external if provided)
        "list_of_author_name": authors,
        "university_or_company_names": f"PROFILE_OWNER: {profile_affiliation}" if profile_affiliation else "",
        "citation_id": citation_id,
        "journal": journal,
        "publication_date": publication_date,
        "citations_by_year": json.dumps(citations_by_year, ensure_ascii=False),
        "resources": json.dumps(resources, ensure_ascii=False),
        "description": description
    })

df2 = pd.DataFrame(rows2)
csv2_path = os.path.join(OUTDIR, "papers.csv")
df2.to_csv(csv2_path, index=False)

print("✅ Done")
print("CSV1:", csv1_path)
print("CSV2:", csv2_path)
print("Total papers:", total_papers)
print("Total citations:", total_citations)





!pip -q install pandas requests rapidfuzz networkx pycountry

import os, re, json, time
import requests
import pandas as pd
import networkx as nx
import pycountry
from rapidfuzz import fuzz

SERPAPI_KEY = os.environ.get("SERPAPI_KEY")  # set this in Colab env or a prior cell
if not SERPAPI_KEY:
    raise RuntimeError("Set SERPAPI_KEY in environment first (os.environ['SERPAPI_KEY']=...).")

AUTHOR_ID = "Mwd7y2QAAAAJ"
OUTDIR = "out_rfe"
os.makedirs(OUTDIR, exist_ok=True)

# ---- throttles (IMPORTANT: SerpApi calls can add up fast) ----
SLEEP_SERPAPI = 0.8
SLEEP_OPENALEX = 0.2

# ---- scope knobs ----
MAX_YOUR_PAPERS = None            # e.g., 20 while testing; None = all
MAX_CITING_RESULTS_PER_PAPER = 200 # caps citing papers fetched per your paper
OPENALEX_MATCH_TOPK = 5           # how many OpenAlex candidates to consider

SERPAPI_ENDPOINT = "https://serpapi.com/search.json"
OPENALEX_WORKS = "https://api.openalex.org/works"
OPENALEX_SOURCES = "https://api.openalex.org/sources"

sess = requests.Session()

def serpapi_get(params: dict) -> dict:
    params = dict(params)
    params["api_key"] = SERPAPI_KEY
    r = sess.get(SERPAPI_ENDPOINT, params=params, timeout=90)
    r.raise_for_status()
    time.sleep(SLEEP_SERPAPI)
    return r.json()

def openalex_get(url: str, params: dict) -> dict:
    r = sess.get(url, params=params, timeout=90)
    r.raise_for_status()
    time.sleep(SLEEP_OPENALEX)
    return r.json()

def extract_cites_id_from_url(url: str) -> str:
    # Scholar cited-by urls often include `cites=1234567890`
    if not url:
        return ""
    m = re.search(r"[?&]cites=(\d+)", url)
    return m.group(1) if m else ""

def country_name_from_code(code: str) -> str:
    if not code:
        return ""
    try:
        return pycountry.countries.get(alpha_2=code).name
    except Exception:
        return code

def safe_get(d, path, default=None):
    cur = d
    for p in path:
        if isinstance(cur, dict) and p in cur:
            cur = cur[p]
        else:
            return default
    return cur

# Pull author page (first page) to get profile + cited-by graph/table
author_payload_first = serpapi_get({
    "engine": "google_scholar_author",
    "author_id": AUTHOR_ID,
    "hl": "en",
    "start": 0,
    "num": 100,
})

author_info = author_payload_first.get("author", {}) or {}
cited_by = author_payload_first.get("cited_by", {}) or {}
cited_by_table = cited_by.get("table", []) or []
cited_by_graph = cited_by.get("graph", []) or []

profile_name = author_info.get("name", "")
profile_affiliation = author_info.get("affiliations", "")

# total citations
total_citations = None
try:
    total_citations = int(cited_by_table[0]["citations"]["all"])
except Exception:
    total_citations = sum(int(x.get("citations", "0")) for x in cited_by_graph if str(x.get("citations","0")).isdigit())

# paginate all your articles
all_articles = []
start = 0
num = 100

while True:
    payload = serpapi_get({
        "engine": "google_scholar_author",
        "author_id": AUTHOR_ID,
        "hl": "en",
        "start": start,
        "num": num
    })
    articles = payload.get("articles", []) or []
    if not articles:
        break
    all_articles.extend(articles)
    if len(articles) < num:
        break
    start += num

if MAX_YOUR_PAPERS is not None:
    all_articles = all_articles[:MAX_YOUR_PAPERS]

print("Profile:", profile_name)
print("Affiliation:", profile_affiliation)
print("Your papers fetched:", len(all_articles))
print("Total citations:", total_citations)

# CSV 1: citations by year
rows1 = []
if cited_by_graph:
    for item in cited_by_graph:
        rows1.append({
            "total_papers": len(all_articles),
            "total_citations": total_citations,
            "year": item.get("year"),
            "citations_in_year": item.get("citations"),
            "profile_name": profile_name,
            "profile_affiliation": profile_affiliation
        })
else:
    rows1.append({
        "total_papers": len(all_articles),
        "total_citations": total_citations,
        "year": "",
        "citations_in_year": "",
        "profile_name": profile_name,
        "profile_affiliation": profile_affiliation
    })

df1 = pd.DataFrame(rows1)
csv1_path = os.path.join(OUTDIR, "profile_citations_by_year.csv")
df1.to_csv(csv1_path, index=False)

# Your papers list (basic)
rows_papers = []
for a in all_articles:
    cited_by_obj = a.get("cited_by", {}) or {}
    rows_papers.append({
        "paper_title": a.get("title", ""),
        "year": a.get("year", ""),
        "authors": a.get("authors", ""),
        "publication": a.get("publication", ""),
        "citations": cited_by_obj.get("value", 0),
        "citation_details_link": cited_by_obj.get("link", ""),
        "cites_id": a.get("cites_id", "") or extract_cites_id_from_url(cited_by_obj.get("link","")),
        "citation_id": a.get("citation_id",""),
        "link": a.get("link",""),
    })

df_papers = pd.DataFrame(rows_papers)
papers_path = os.path.join(OUTDIR, "papers.csv")
df_papers.to_csv(papers_path, index=False)

print("✅ Wrote", csv1_path)
print("✅ Wrote", papers_path)

def fetch_all_citing_results(cites_id: str, limit: int = 200) -> list:
    results = []
    start = 0
    while True:
        payload = serpapi_get({
            "engine": "google_scholar",
            "hl": "en",
            "cites": cites_id,
            "start": start
        })
        organic = payload.get("organic_results", []) or []
        if not organic:
            break
        results.extend(organic)
        if len(results) >= limit:
            return results[:limit]
        # Scholar pages are typically 10 results per page
        if len(organic) < 10:
            break
        start += 10
    return results

# Build citation edges: citing paper -> your paper
edges = []   # for CSV / graph
for idx, p in df_papers.iterrows():
    your_title = p["paper_title"]
    cites_id = str(p.get("cites_id") or "").strip()

    if not cites_id:
        continue

    citing = fetch_all_citing_results(cites_id, limit=MAX_CITING_RESULTS_PER_PAPER)

    for r in citing:
        edges.append({
            "your_paper_title": your_title,
            "your_paper_year": p.get("year",""),
            "cites_id": cites_id,
            "citing_title": r.get("title",""),
            "citing_link": r.get("link",""),
            "citing_snippet": r.get("snippet",""),
            "citing_result_id": r.get("result_id",""),
            "citing_publication_info": json.dumps(r.get("publication_info", {}), ensure_ascii=False),
            "citing_year": safe_get(r, ["publication_info","summary"], "")  # best-effort string
        })

df_edges = pd.DataFrame(edges)
edges_path = os.path.join(OUTDIR, "citation_edges_scholar_exact.csv")
df_edges.to_csv(edges_path, index=False)
print("✅ Wrote", edges_path, "| edges:", len(df_edges))

G = nx.DiGraph()

for _, e in df_edges.iterrows():
    citing = e["citing_title"]
    cited = e["your_paper_title"]
    if citing and cited:
        G.add_node(citing, kind="citing_paper")
        G.add_node(cited, kind="your_paper")
        G.add_edge(citing, cited)

graphml_path = os.path.join(OUTDIR, "citation_map.graphml")
nx.write_graphml(G, graphml_path)
print("✅ Wrote", graphml_path, "| nodes:", G.number_of_nodes(), "| edges:", G.number_of_edges())

openalex_cache = {}

def openalex_match_work(title: str, year_hint=None):
    title = (title or "").strip()
    if not title:
        return None

    key = (title.lower(), str(year_hint or ""))
    if key in openalex_cache:
        return openalex_cache[key]

    data = openalex_get(OPENALEX_WORKS, params={
        "search": title,
        "per_page": OPENALEX_MATCH_TOPK
    })
    candidates = data.get("results", []) or []

    best = None
    best_score = -1
    for w in candidates:
        w_title = (w.get("display_name") or "")
        score = fuzz.token_set_ratio(title.lower(), w_title.lower())
        # small boost if year matches
        w_year = w.get("publication_year")
        if year_hint and w_year == year_hint:
            score += 5
        if score > best_score:
            best_score = score
            best = w

    # require a reasonable match
    if best_score < 70:
        best = None

    openalex_cache[key] = best
    return best

# Aggregations
country_counts = {}
institution_counts = {}
author_counts = {}

# Per-edge enriched table
enriched_rows = []

for _, e in df_edges.iterrows():
    citing_title = e["citing_title"]
    if not citing_title:
        continue

    w = openalex_match_work(citing_title)
    institutions = []
    countries = []
    authors = []

    if w:
        # institutions from authorships
        for auth in w.get("authorships", []) or []:
            a = auth.get("author", {}) or {}
            if a.get("display_name"):
                authors.append(a["display_name"])

            for inst in auth.get("institutions", []) or []:
                inst_name = inst.get("display_name","")
                cc = inst.get("country_code","") or ""
                if inst_name:
                    institutions.append((inst_name, cc))
                    if cc:
                        countries.append(cc)

        # aggregate counts
        for cc in countries:
            country_counts[cc] = country_counts.get(cc, 0) + 1

        for inst_name, cc in institutions:
            key = (inst_name, cc)
            institution_counts[key] = institution_counts.get(key, 0) + 1

        for a in authors:
            author_counts[a] = author_counts.get(a, 0) + 1

    enriched_rows.append({
        **e.to_dict(),
        "openalex_work_id": (w.get("id") if w else ""),
        "openalex_match_score_note": ("matched" if w else "no_match"),
        "institutions": json.dumps([{"name": n, "country_code": cc} for (n,cc) in institutions], ensure_ascii=False),
        "countries": json.dumps(sorted(set(countries)), ensure_ascii=False),
        "authors": json.dumps(sorted(set(authors)), ensure_ascii=False),
    })

df_enriched = pd.DataFrame(enriched_rows)
enriched_path = os.path.join(OUTDIR, "citation_edges_enriched_openalex.csv")
df_enriched.to_csv(enriched_path, index=False)
print("✅ Wrote", enriched_path)

# (b) citations by country
df_country = pd.DataFrame([
    {"country_code": cc, "country": country_name_from_code(cc), "citations": cnt}
    for cc, cnt in country_counts.items()
]).sort_values("citations", ascending=False)

country_path = os.path.join(OUTDIR, "citations_by_country.csv")
df_country.to_csv(country_path, index=False)

# (c) citations by institution
df_inst = pd.DataFrame([
    {"institution": inst, "country_code": cc, "country": country_name_from_code(cc), "citations": cnt}
    for (inst, cc), cnt in institution_counts.items()
]).sort_values("citations", ascending=False)

inst_path = os.path.join(OUTDIR, "citations_by_institution.csv")
df_inst.to_csv(inst_path, index=False)

# (d) prominent researchers
df_auth = pd.DataFrame([
    {"researcher": a, "citations": cnt}
    for a, cnt in author_counts.items()
]).sort_values("citations", ascending=False)

auth_path = os.path.join(OUTDIR, "top_citing_researchers.csv")
df_auth.to_csv(auth_path, index=False)

print("✅ Wrote", country_path)
print("✅ Wrote", inst_path)
print("✅ Wrote", auth_path)

df_country.head(15), df_inst.head(15), df_auth.head(20)

# Pick top N citing examples per your paper
N_EXAMPLES_PER_YOUR_PAPER = 3

examples = []
for your_title, sub in df_enriched.groupby("your_paper_title"):
    # rank by whether matched + has institutions + has snippet
    sub2 = sub.copy()
    sub2["has_inst"] = sub2["institutions"].apply(lambda s: 1 if s and s != "[]" else 0)
    sub2["has_snip"] = sub2["citing_snippet"].apply(lambda s: 1 if isinstance(s,str) and len(s.strip())>0 else 0)
    sub2 = sub2.sort_values(["has_inst","has_snip"], ascending=False).head(50)

    # take top N unique citing titles
    seen = set()
    count = 0
    for _, r in sub2.iterrows():
        ct = r["citing_title"]
        if not ct or ct in seen:
            continue
        seen.add(ct)
        examples.append({
            "your_paper_title": your_title,
            "citing_paper_title": ct,
            "citing_link": r.get("citing_link",""),
            "snippet_quote_best_effort": r.get("citing_snippet",""),
            "institutions_best_effort": r.get("institutions",""),
            "countries_best_effort": r.get("countries",""),
        })
        count += 1
        if count >= N_EXAMPLES_PER_YOUR_PAPER:
            break

df_examples = pd.DataFrame(examples)
examples_path = os.path.join(OUTDIR, "prominent_citation_examples.csv")
df_examples.to_csv(examples_path, index=False)
print("✅ Wrote", examples_path)
df_examples.head(20)

source_cache = {}

def openalex_find_source(venue_name: str):
    venue_name = (venue_name or "").strip()
    if not venue_name:
        return None
    key = venue_name.lower()
    if key in source_cache:
        return source_cache[key]

    data = openalex_get(OPENALEX_SOURCES, params={"search": venue_name, "per_page": 5})
    cand = data.get("results", []) or []

    best = None
    best_score = -1
    for s in cand:
        s_name = s.get("display_name","")
        score = fuzz.token_set_ratio(venue_name.lower(), s_name.lower())
        if score > best_score:
            best_score = score
            best = s

    if best_score < 70:
        best = None

    source_cache[key] = best
    return best

venue_rows = []
for _, p in df_papers.iterrows():
    venue_raw = p.get("publication","")
    if not venue_raw:
        continue
    # take first chunk as a venue guess
    venue_guess = venue_raw.split(",")[0].strip()

    s = openalex_find_source(venue_guess)
    if not s:
        venue_rows.append({
            "venue": venue_guess,
            "openalex_source_id": "",
            "impact_factor_like_2yr_mean_citedness": "",
            "h_index": "",
            "i10_index": "",
            "works_count": "",
            "cited_by_count": "",
        })
        continue

    stats = s.get("summary_stats", {}) or {}
    venue_rows.append({
        "venue": venue_guess,
        "openalex_source_id": s.get("id",""),
        "impact_factor_like_2yr_mean_citedness": stats.get("2yr_mean_citedness",""),
        "h_index": stats.get("h_index",""),
        "i10_index": stats.get("i10_index",""),
        "works_count": s.get("works_count",""),
        "cited_by_count": s.get("cited_by_count",""),
    })

df_venue = pd.DataFrame(venue_rows).drop_duplicates(subset=["venue","openalex_source_id"])
venue_path = os.path.join(OUTDIR, "venue_metrics_openalex.csv")
df_venue.to_csv(venue_path, index=False)

print("✅ Wrote", venue_path)
df_venue.head(20)







# ===========================
# ONE-CELL END-TO-END (Colab)
# Scholar-exact (SerpApi) + Enrichment (OpenAlex)
# Outputs (raw + filtered) + charts + zip download
# ===========================

# --- 0) Install deps ---
!pip -q install pandas requests rapidfuzz networkx pycountry matplotlib

import os, re, json, time, hashlib, math, zipfile
from urllib.parse import urlparse, parse_qs

import requests
import pandas as pd
import matplotlib.pyplot as plt

from rapidfuzz import fuzz
import pycountry
import networkx as nx
from google.colab import files

# ---------------------------
# 1) USER CONFIG
# ---------------------------
AUTHOR_ID = "Mwd7y2QAAAAJ"     # <-- Google Scholar profile id (user=...)
OUTDIR    = "scholar_rfe_outputs"
CACHE_DIR = os.path.join(OUTDIR, "cache")

# IMPORTANT: set your SerpApi key in one of these ways:
#   - os.environ["SERPAPI_KEY"] = "YOUR_KEY"
#   - or in Colab: Runtime -> Secrets / Environment (recommended)
SERPAPI_KEY = os.environ.get("SERPAPI_KEY", "").strip()
if not SERPAPI_KEY:
    raise RuntimeError("Missing SERPAPI_KEY. Set os.environ['SERPAPI_KEY']='YOUR_KEY' and rerun this cell.")

# Throttles (adjust if needed)
SLEEP_SERPAPI  = 0.8
SLEEP_OPENALEX = 0.25

# Limits (set to None for full; use small numbers while testing)
MAX_YOUR_PAPERS              = None      # e.g., 20 for testing
MAX_CITING_RESULTS_PER_PAPER = None      # e.g., 200 for testing (None = fetch all pages)

# OpenAlex matching controls
OPENALEX_MATCH_TOPK     = 5
OPENALEX_MIN_MATCH      = 72   # raise if too many wrong matches; lower if too many "no_match"

os.makedirs(CACHE_DIR, exist_ok=True)

SERPAPI_ENDPOINT = "https://serpapi.com/search.json"
OPENALEX_WORKS   = "https://api.openalex.org/works"
OPENALEX_SOURCES = "https://api.openalex.org/sources"

sess = requests.Session()

# ---------------------------
# 2) HELPERS
# ---------------------------
def _cache_path(name: str) -> str:
    return os.path.join(CACHE_DIR, name)

def load_cache(name: str):
    p = _cache_path(name)
    if os.path.exists(p):
        with open(p, "r", encoding="utf-8") as f:
            return json.load(f)
    return None

def save_cache(name: str, obj):
    p = _cache_path(name)
    with open(p, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False)

def serpapi_get(params: dict, cache_key: str = None) -> dict:
    if cache_key:
        cached = load_cache(cache_key)
        if cached is not None:
            return cached

    params = dict(params)
    params["api_key"] = SERPAPI_KEY
    r = sess.get(SERPAPI_ENDPOINT, params=params, timeout=120)
    r.raise_for_status()
    data = r.json()
    time.sleep(SLEEP_SERPAPI)

    if cache_key:
        save_cache(cache_key, data)
    return data

def openalex_get(url: str, params: dict, cache_key: str = None) -> dict:
    if cache_key:
        cached = load_cache(cache_key)
        if cached is not None:
            return cached

    r = sess.get(url, params=params, timeout=120)
    r.raise_for_status()
    data = r.json()
    time.sleep(SLEEP_OPENALEX)

    if cache_key:
        save_cache(cache_key, data)
    return data

def normalize_title(t: str) -> str:
    t = (t or "").lower().strip()
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"[^\w\s]", "", t)
    return t

def stable_citing_key(result: dict) -> str:
    rid = result.get("result_id")
    if rid:
        return f"rid:{rid}"

    title = normalize_title(result.get("title", ""))
    link = result.get("link", "") or ""
    try:
        u = urlparse(link)
        link_sig = f"{u.netloc}{u.path}"
    except Exception:
        link_sig = link

    year = ""
    summ = (result.get("publication_info", {}) or {}).get("summary", "") or ""
    m = re.search(r"\b(19|20)\d{2}\b", summ)
    if m:
        year = m.group(0)

    raw = f"{title}|{year}|{link_sig}"
    h = hashlib.sha1(raw.encode("utf-8")).hexdigest()[:16]
    return f"hash:{h}"

def extract_cites_id(url: str) -> str:
    if not url:
        return ""
    try:
        q = parse_qs(urlparse(url).query)
        if "cites" in q and q["cites"]:
            return q["cites"][0]
    except Exception:
        pass
    m = re.search(r"[?&]cites=(\d+)", url)
    return m.group(1) if m else ""

def country_name_from_code(code: str) -> str:
    if not code:
        return ""
    try:
        c = pycountry.countries.get(alpha_2=code)
        return c.name if c else code
    except Exception:
        return code

def venue_guess_from_publication(pub: str) -> str:
    if not pub:
        return ""
    # often first chunk resembles venue
    return pub.split(",")[0].strip()

# ---------------------------
# 3) FETCH PROFILE + ALL YOUR PAPERS (Scholar-exact via SerpApi)
# ---------------------------
print("==> Fetching author profile + paper list (Scholar-exact via SerpApi)...")

# First page also includes cited_by graph/table
author_first = serpapi_get({
    "engine": "google_scholar_author",
    "author_id": AUTHOR_ID,
    "hl": "en",
    "start": 0,
    "num": 100
}, cache_key=f"author_first_{AUTHOR_ID}.json")

author_info = author_first.get("author", {}) or {}
cited_by    = author_first.get("cited_by", {}) or {}
graph       = cited_by.get("graph", []) or []
table       = cited_by.get("table", []) or []

profile_name        = author_info.get("name", "")
profile_affiliation = author_info.get("affiliations", "")

# total citations
try:
    total_citations = int(table[0]["citations"]["all"])
except Exception:
    total_citations = sum(int(x.get("citations", "0")) for x in graph if str(x.get("citations","0")).isdigit())

# paginate all your papers
your_articles = []
start = 0
num = 100
while True:
    ck = f"author_articles_{AUTHOR_ID}_start{start}_num{num}.json"
    payload = serpapi_get({
        "engine": "google_scholar_author",
        "author_id": AUTHOR_ID,
        "hl": "en",
        "start": start,
        "num": num
    }, cache_key=ck)

    arts = payload.get("articles", []) or []
    if not arts:
        break

    your_articles.extend(arts)
    if len(arts) < num:
        break
    start += num

if MAX_YOUR_PAPERS is not None:
    your_articles = your_articles[:MAX_YOUR_PAPERS]

print(f"Profile: {profile_name}")
print(f"Affiliation: {profile_affiliation}")
print(f"Your papers: {len(your_articles)} | Total citations (profile): {total_citations}")

# Save citations by year (raw)
df_cites_year = pd.DataFrame([{
    "profile_name": profile_name,
    "profile_affiliation": profile_affiliation,
    "total_papers": len(your_articles),
    "total_citations": total_citations,
    "year": g.get("year"),
    "citations_in_year": g.get("citations")
} for g in graph] if graph else [{
    "profile_name": profile_name,
    "profile_affiliation": profile_affiliation,
    "total_papers": len(your_articles),
    "total_citations": total_citations,
    "year": "",
    "citations_in_year": ""
}])

os.makedirs(OUTDIR, exist_ok=True)
df_cites_year.to_csv(os.path.join(OUTDIR, "profile_citations_by_year.csv"), index=False)

# Save your papers list (raw)
your_rows = []
for a in your_articles:
    cited_by_obj = a.get("cited_by", {}) or {}
    cites_link = cited_by_obj.get("link", "")
    cites_id = a.get("cites_id", "") or extract_cites_id(cites_link)
    your_rows.append({
        "your_paper_title": a.get("title", ""),
        "your_paper_year": a.get("year", ""),
        "your_paper_authors": a.get("authors", ""),
        "your_paper_publication_raw": a.get("publication", ""),
        "your_paper_venue_guess": venue_guess_from_publication(a.get("publication","")),
        "your_paper_citations": cited_by_obj.get("value", 0),
        "citation_details_link": cites_link,
        "cites_id": cites_id,
        "citation_id": a.get("citation_id",""),
        "your_paper_link": a.get("link",""),
    })

df_your = pd.DataFrame(your_rows)
df_your.to_csv(os.path.join(OUTDIR, "your_papers.csv"), index=False)

# ---------------------------
# 4) FETCH ALL CITING PAPERS (RAW) + EDGES + MASTER DEDUPE
# ---------------------------
print("\n==> Fetching ALL citing papers for each of your papers (Scholar-exact cites=...)")

def fetch_all_citing_results(cites_id: str, hard_limit: int = None) -> list:
    results = []
    start = 0
    while True:
        ck = f"cites_{cites_id}_start{start}.json"
        payload = serpapi_get({
            "engine": "google_scholar",
            "hl": "en",
            "cites": cites_id,
            "start": start
        }, cache_key=ck)

        organic = payload.get("organic_results", []) or []
        if not organic:
            break

        results.extend(organic)
        if hard_limit is not None and len(results) >= hard_limit:
            return results[:hard_limit]

        if len(organic) < 10:
            break
        start += 10

    return results

edges = []
per_your_paper = []
master_map = {}  # citing_key -> canonical citing record

expected_sum = 0
fetched_total = 0

for idx, row in df_your.iterrows():
    your_title = row["your_paper_title"]
    your_year  = row.get("your_paper_year","")
    cites_id   = str(row.get("cites_id") or "").strip()
    cite_count = int(row.get("your_paper_citations") or 0)
    if not cites_id:
        continue

    expected_sum += cite_count
    citing_list = fetch_all_citing_results(cites_id, hard_limit=MAX_CITING_RESULTS_PER_PAPER)
    fetched_total += len(citing_list)

    for r in citing_list:
        key = stable_citing_key(r)

        pubinfo = r.get("publication_info", {}) or {}
        summ = pubinfo.get("summary","") or ""
        authors_struct = pubinfo.get("authors", []) or []
        author_names = []
        for a in authors_struct:
            if isinstance(a, dict) and a.get("name"):
                author_names.append(a["name"])

        per_your_paper.append({
            "your_paper_title": your_title,
            "your_paper_year": your_year,
            "cites_id": cites_id,

            "citing_key": key,
            "citing_title": r.get("title",""),
            "citing_link": r.get("link",""),
            "citing_snippet": r.get("snippet",""),
            "citing_publication_summary": summ,
            "citing_author_names": "; ".join(author_names),
            "citing_result_id": r.get("result_id",""),
            "raw_publication_info_json": json.dumps(pubinfo, ensure_ascii=False),
        })

        edges.append({
            "citing_key": key,
            "citing_title": r.get("title",""),
            "your_paper_title": your_title,
            "your_paper_year": your_year,
            "cites_id": cites_id,
        })

        if key not in master_map:
            master_map[key] = {
                "citing_key": key,
                "citing_title": r.get("title",""),
                "citing_link": r.get("link",""),
                "citing_snippet": r.get("snippet",""),
                "citing_publication_summary": summ,
                "citing_author_names": "; ".join(author_names),
                "citing_result_id": r.get("result_id",""),
                "raw_publication_info_json": json.dumps(pubinfo, ensure_ascii=False),
            }

    if (idx + 1) % 5 == 0:
        print(f"  processed {idx+1}/{len(df_your)} | fetched rows: {fetched_total} | unique citing: {len(master_map)}")

df_edges  = pd.DataFrame(edges).drop_duplicates()
df_per    = pd.DataFrame(per_your_paper)
df_master = pd.DataFrame(list(master_map.values()))

df_edges.to_csv(os.path.join(OUTDIR, "citation_edges_full.csv"), index=False)
df_per.to_csv(os.path.join(OUTDIR, "citing_papers_per_your_paper_raw.csv"), index=False)
df_master.to_csv(os.path.join(OUTDIR, "citing_papers_master_raw.csv"), index=False)

print("\nRAW CITATION SET SUMMARY")
print("  Sum of your paper citation counts (expected upper bound):", expected_sum)
print("  Citing rows fetched (with duplicates across your papers):", len(df_per))
print("  Unique citing papers (master):", len(df_master))

# ---------------------------
# 5) ENRICH CITING PAPERS (OpenAlex) for:
#    1) citations by country
#    2) citations by institution/company (country)
#    3) prominent researchers/institutions
# ---------------------------
print("\n==> Enriching citing papers with OpenAlex (institutions/countries/authors + paper impact proxy)...")

openalex_work_cache = {}

def openalex_match_work(title: str):
    title = (title or "").strip()
    if not title:
        return None, 0

    k = f"openalex_work_search_{hashlib.sha1(title.lower().encode()).hexdigest()[:16]}.json"
    data = openalex_get(OPENALEX_WORKS, params={
        "search": title,
        "per_page": OPENALEX_MATCH_TOPK
    }, cache_key=k)

    candidates = data.get("results", []) or []
    best = None
    best_score = -1

    for w in candidates:
        wtitle = (w.get("display_name") or "")
        score = fuzz.token_set_ratio(title.lower(), wtitle.lower())
        if score > best_score:
            best_score = score
            best = w

    if best_score < OPENALEX_MIN_MATCH:
        return None, best_score

    return best, best_score

country_counts = {}
institution_counts = {}
researcher_counts = {}
prominent_institution_counts = {}  # weighted by citing work cited_by_count (proxy)

enriched_rows = []

for i, r in df_master.iterrows():
    title = r["citing_title"]
    w, score = openalex_match_work(title)

    institutions = []
    countries = []
    authors = []
    cited_by_count = None
    source_name = ""
    source_id = ""
    pub_year = None

    if w:
        cited_by_count = w.get("cited_by_count", 0)
        pub_year = w.get("publication_year", None)

        # source / venue where available
        primary_loc = w.get("primary_location", {}) or {}
        src = primary_loc.get("source", {}) or {}
        source_name = src.get("display_name","") or ""
        source_id = src.get("id","") or ""

        # authorships -> institutions/countries + author names
        for auth in w.get("authorships", []) or []:
            a = auth.get("author", {}) or {}
            if a.get("display_name"):
                authors.append(a["display_name"])

            for inst in auth.get("institutions", []) or []:
                inst_name = inst.get("display_name","")
                cc = inst.get("country_code","") or ""
                if inst_name:
                    institutions.append((inst_name, cc))
                    if cc:
                        countries.append(cc)

        # Aggregations (counting each citing paper once per country/institution occurrence)
        for cc in set(countries):
            country_counts[cc] = country_counts.get(cc, 0) + 1

        for inst_name, cc in set(institutions):
            key = (inst_name, cc)
            institution_counts[key] = institution_counts.get(key, 0) + 1
            # prominence proxy: sum of citing-work's citations in OpenAlex
            prominent_institution_counts[key] = prominent_institution_counts.get(key, 0) + (cited_by_count or 0)

        for a in set(authors):
            researcher_counts[a] = researcher_counts.get(a, 0) + 1

    enriched_rows.append({
        **r.to_dict(),
        "openalex_matched": bool(w),
        "openalex_match_score": score,
        "openalex_work_id": (w.get("id") if w else ""),
        "openalex_publication_year": pub_year if pub_year is not None else "",
        "openalex_cited_by_count": cited_by_count if cited_by_count is not None else "",
        "openalex_source_name": source_name,
        "openalex_source_id": source_id,
        "openalex_institutions": json.dumps([{"name": n, "country_code": cc} for (n,cc) in institutions], ensure_ascii=False),
        "openalex_countries": json.dumps(sorted(set(countries)), ensure_ascii=False),
        "openalex_authors": json.dumps(sorted(set(authors)), ensure_ascii=False),
    })

df_enriched = pd.DataFrame(enriched_rows)
df_enriched.to_csv(os.path.join(OUTDIR, "citing_papers_master_enriched_openalex.csv"), index=False)

# ---------------------------
# 6) (1) CITATIONS BY COUNTRY (sorted) + CHART
# ---------------------------
df_country = pd.DataFrame([{
    "country_code": cc,
    "country": country_name_from_code(cc),
    "citing_papers": cnt
} for cc, cnt in country_counts.items()]).sort_values("citing_papers", ascending=False)

df_country.to_csv(os.path.join(OUTDIR, "citations_by_country.csv"), index=False)

# chart (top 20)
topN = 20
df_country_top = df_country.head(topN).copy()
plt.figure(figsize=(10, 6))
plt.bar(df_country_top["country"], df_country_top["citing_papers"])
plt.xticks(rotation=75, ha="right")
plt.title("Citations by Country (count of citing papers, Scholar-exact set)")
plt.tight_layout()
country_png = os.path.join(OUTDIR, "citations_by_country_top20.png")
plt.savefig(country_png, dpi=200)
plt.close()

# ---------------------------
# 7) (2) CITATIONS BY INSTITUTION/COMPANY (COUNTRY) (sorted) + CHART
# ---------------------------
df_inst = pd.DataFrame([{
    "institution_or_company": inst,
    "country_code": cc,
    "country": country_name_from_code(cc),
    "citing_papers": cnt,
    "prominence_proxy_sum_cited_by_count": prominent_institution_counts.get((inst, cc), 0)
} for (inst, cc), cnt in institution_counts.items()]).sort_values(
    ["citing_papers", "prominence_proxy_sum_cited_by_count"],
    ascending=False
)

df_inst.to_csv(os.path.join(OUTDIR, "citations_by_institution_company.csv"), index=False)

# chart (top 20)
df_inst_top = df_inst.head(topN).copy()
labels = [f"{r['institution_or_company']} ({r['country_code']})" if r["country_code"] else r["institution_or_company"]
          for _, r in df_inst_top.iterrows()]

plt.figure(figsize=(12, 7))
plt.bar(labels, df_inst_top["citing_papers"])
plt.xticks(rotation=80, ha="right")
plt.title("Citations by Institution/Company (Country)\n(count of citing papers, Scholar-exact set)")
plt.tight_layout()
inst_png = os.path.join(OUTDIR, "citations_by_institution_company_top20.png")
plt.savefig(inst_png, dpi=200)
plt.close()

# ---------------------------
# 8) (3) HIGHLIGHT PROMINENT CITERS (researchers + institutions) + EXAMPLES
# ---------------------------
# Most prominent researchers by count of appearances across matched citing works
df_top_researchers = pd.DataFrame([
    {"researcher": a, "citing_papers": cnt}
    for a, cnt in researcher_counts.items()
]).sort_values("citing_papers", ascending=False)

df_top_researchers.to_csv(os.path.join(OUTDIR, "top_citing_researchers.csv"), index=False)

# Most prominent institutions by (count, then prominence proxy)
df_top_institutions = df_inst.head(50).copy()
df_top_institutions.to_csv(os.path.join(OUTDIR, "top_citing_institutions.csv"), index=False)

# Build examples:
# For each of your papers, pick top citing papers by OpenAlex cited_by_count (impact proxy) + include Scholar snippet.
df_edge_join = df_edges.merge(df_enriched[[
    "citing_key","openalex_cited_by_count","openalex_institutions","openalex_countries","openalex_authors","citing_snippet","citing_link"
]], on="citing_key", how="left")

df_edge_join["openalex_cited_by_count_num"] = pd.to_numeric(df_edge_join["openalex_cited_by_count"], errors="coerce").fillna(0)

EXAMPLES_PER_YOUR_PAPER = 3
examples = []

for your_title, sub in df_edge_join.groupby("your_paper_title"):
    sub = sub.sort_values("openalex_cited_by_count_num", ascending=False).head(40)
    seen = set()
    picked = 0
    for _, rr in sub.iterrows():
        ct = rr.get("citing_title","")
        if not ct or ct in seen:
            continue
        seen.add(ct)
        examples.append({
            "your_paper_title": your_title,
            "citing_paper_title": ct,
            "citing_link": rr.get("citing_link",""),
            "snippet_quote_best_effort": rr.get("citing_snippet",""),
            "openalex_cited_by_count_proxy": int(rr.get("openalex_cited_by_count_num", 0)),
            "openalex_institutions_best_effort": rr.get("openalex_institutions",""),
            "openalex_countries_best_effort": rr.get("openalex_countries",""),
            "openalex_authors_best_effort": rr.get("openalex_authors",""),
        })
        picked += 1
        if picked >= EXAMPLES_PER_YOUR_PAPER:
            break

df_examples = pd.DataFrame(examples)
df_examples.to_csv(os.path.join(OUTDIR, "prominent_citation_examples.csv"), index=False)

# ---------------------------
# 9) (4) VENUE PROMINENCE METRICS
# IMPORTANT NOTE:
# - True Impact Factor / CiteScore / CiteScore rank / h5 index & median are proprietary and not reliably available
#   from Google Scholar pages via SerpApi in a standardized way.
# - This code provides OPEN metrics from OpenAlex sources:
#     - h_index, i10_index, 2yr_mean_citedness (impact-factor-like proxy), works_count, cited_by_count
# - We output the requested columns, leaving proprietary ones blank ("") unless you enrich them separately.
# ---------------------------
print("\n==> Computing venue metrics (OpenAlex source stats; proprietary metrics left blank)...")

source_cache = {}

def openalex_find_source(venue_name: str):
    venue_name = (venue_name or "").strip()
    if not venue_name:
        return None, 0

    key = f"openalex_source_search_{hashlib.sha1(venue_name.lower().encode()).hexdigest()[:16]}.json"
    data = openalex_get(OPENALEX_SOURCES, params={"search": venue_name, "per_page": 5}, cache_key=key)
    cand = data.get("results", []) or []
    best = None
    best_score = -1
    for s in cand:
        sname = s.get("display_name","") or ""
        score = fuzz.token_set_ratio(venue_name.lower(), sname.lower())
        if score > best_score:
            best_score = score
            best = s
    if best_score < 70:
        return None, best_score
    return best, best_score

venues = sorted(set([v for v in df_your["your_paper_venue_guess"].fillna("").tolist() if v.strip()]))
venue_rows = []

for v in venues:
    s, sc = openalex_find_source(v)
    if not s:
        venue_rows.append({
            "Venue (Journal/Conference)": v,
            "Impact factor": "",          # proprietary
            "CiteScore": "",              # proprietary
            "CiteScore rank within category": "",  # proprietary
            "h-5 Index": "",              # proprietary (Google Scholar Metrics)
            "h-5 Median": "",             # proprietary (Google Scholar Metrics)

            # Open alternatives (filled)
            "OpenAlex 2yr_mean_citedness (impact-like)": "",
            "OpenAlex h_index": "",
            "OpenAlex i10_index": "",
            "OpenAlex works_count": "",
            "OpenAlex cited_by_count": "",
            "OpenAlex source_id": "",
            "OpenAlex match_score": sc
        })
        continue

    stats = s.get("summary_stats", {}) or {}
    venue_rows.append({
        "Venue (Journal/Conference)": v,

        "Impact factor": "",  # proprietary
        "CiteScore": "",      # proprietary
        "CiteScore rank within category": "",  # proprietary
        "h-5 Index": "",      # proprietary
        "h-5 Median": "",     # proprietary

        "OpenAlex 2yr_mean_citedness (impact-like)": stats.get("2yr_mean_citedness",""),
        "OpenAlex h_index": stats.get("h_index",""),
        "OpenAlex i10_index": stats.get("i10_index",""),
        "OpenAlex works_count": s.get("works_count",""),
        "OpenAlex cited_by_count": s.get("cited_by_count",""),
        "OpenAlex source_id": s.get("id",""),
        "OpenAlex match_score": sc
    })

df_venue_metrics = pd.DataFrame(venue_rows).sort_values(
    ["OpenAlex h_index","OpenAlex 2yr_mean_citedness (impact-like)"],
    ascending=False,
    na_position="last"
)
df_venue_metrics.to_csv(os.path.join(OUTDIR, "venue_prominence_metrics.csv"), index=False)

# ---------------------------
# 10) SAVE "FILTERED" TABLES in the exact RFE-friendly layout (top 18 as in template)
# ---------------------------
df_country_top18 = df_country.head(18).copy()
df_country_top18.to_csv(os.path.join(OUTDIR, "RFE_country_top18.csv"), index=False)

df_inst_top18 = df_inst.head(18).copy()
df_inst_top18.to_csv(os.path.join(OUTDIR, "RFE_institution_top18.csv"), index=False)

df_top_researchers_30 = df_top_researchers.head(30).copy()
df_top_researchers_30.to_csv(os.path.join(OUTDIR, "RFE_top_researchers_top30.csv"), index=False)

df_examples.to_csv(os.path.join(OUTDIR, "RFE_prominent_citation_examples.csv"), index=False)
df_venue_metrics.to_csv(os.path.join(OUTDIR, "RFE_venue_prominence_metrics.csv"), index=False)

# ---------------------------
# 11) OPTIONAL: CITATION MAP GRAPH (GraphML)
# ---------------------------
G = nx.DiGraph()
for _, e in df_edges.iterrows():
    citing = e.get("citing_title","")
    cited  = e.get("your_paper_title","")
    if citing and cited:
        G.add_node(citing, kind="citing_paper")
        G.add_node(cited,  kind="your_paper")
        G.add_edge(citing, cited)

graphml_path = os.path.join(OUTDIR, "citation_map.graphml")
nx.write_graphml(G, graphml_path)

# ---------------------------
# 12) QUICK CONSOLE SUMMARY
# ---------------------------
print("\n==================== SUMMARY ====================")
print("Outputs folder:", OUTDIR)
print("Raw files:")
print(" - your_papers.csv")
print(" - citing_papers_per_your_paper_raw.csv")
print(" - citing_papers_master_raw.csv")
print(" - citation_edges_full.csv")
print(" - citing_papers_master_enriched_openalex.csv")
print("Filtered/RFE files:")
print(" - citations_by_country.csv + citations_by_country_top20.png + RFE_country_top18.csv")
print(" - citations_by_institution_company.csv + citations_by_institution_company_top20.png + RFE_institution_top18.csv")
print(" - top_citing_researchers.csv + RFE_top_researchers_top30.csv")
print(" - prominent_citation_examples.csv + RFE_prominent_citation_examples.csv")
print(" - venue_prominence_metrics.csv + RFE_venue_prominence_metrics.csv")
print(" - citation_map.graphml")
print("\nNOTES:")
print(" - Country/Institution charts are derived from OpenAlex enrichment of the Scholar-exact citing set.")
print(" - Proprietary metrics (Impact Factor, CiteScore, CiteScore rank, h5 index/median) are left blank by default.")
print("   The file includes OpenAlex open alternatives (2yr_mean_citedness, h_index, i10_index).")

# ---------------------------
# 13) ZIP + DOWNLOAD EVERYTHING
# ---------------------------
zip_path = f"{OUTDIR}.zip"
with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as z:
    for root, dirs, files_ in os.walk(OUTDIR):
        for fn in files_:
            full = os.path.join(root, fn)
            rel  = os.path.relpath(full, OUTDIR)
            z.write(full, arcname=rel)

print("\n✅ Created zip:", zip_path)
files.download(zip_path)

# OPTIONAL: also download the two chart PNGs directly
files.download(os.path.join(OUTDIR, "citations_by_country_top20.png"))
files.download(os.path.join(OUTDIR, "citations_by_institution_company_top20.png"))



# ============================================================
# ADD-ON CELL (run AFTER the previous pipeline cell)
# Creates WORLD MAPS + extra graphs from the saved OUTDIR data
# ============================================================

!pip -q install plotly kaleido rapidfuzz

import os, json, time, hashlib
import pandas as pd
import requests
from rapidfuzz import fuzz
import plotly.express as px
import plotly.graph_objects as go
from google.colab import files
import zipfile

# --------------------------
# CONFIG: point this to the same OUTDIR you used before
# --------------------------
OUTDIR = "scholar_rfe_outputs"     # <-- must match your earlier cell output folder
MAPDIR = os.path.join(OUTDIR, "maps")
CACHEDIR = os.path.join(OUTDIR, "cache_geo")
os.makedirs(MAPDIR, exist_ok=True)
os.makedirs(CACHEDIR, exist_ok=True)

SLEEP_OPENALEX = 0.25
OPENALEX_INSTITUTIONS = "https://api.openalex.org/institutions"
OPENALEX_AUTHORS      = "https://api.openalex.org/authors"

# Define MAILTO here (it was previously defined in KS8erDXYPNDe which was deleted)
MAILTO = "nppatel7@asu.edu"

sess = requests.Session()

def _cache_path(name: str) -> str:
    return os.path.join(CACHEDIR, name)

def load_cache(name: str):
    p = _cache_path(name)
    if os.path.exists(p):
        with open(p, "r", encoding="utf-8") as f:
            return json.load(f)
    return None

def save_cache(name: str, obj):
    p = _cache_path(name)
    with open(p, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False)

def openalex_get(url: str, params: dict, cache_key: str = None):
    if cache_key:
        cached = load_cache(cache_key)
        if cached is not None:
            return cached

    # Ensure mailto is always included for polite pool access
    call_params = dict(params)
    call_params["mailto"] = MAILTO

    r = sess.get(url, params=call_params, timeout=120)
    r.raise_for_status()

    data = None
    # Robustly check for JSON content type before decoding
    if 'application/json' in r.headers.get('Content-Type', '').lower():
        try:
            data = r.json()
        except json.JSONDecodeError:
            # Content-Type was JSON, but actual content was not valid JSON
            print(f"Warning: OpenAlex API for {url} returned malformed JSON despite Content-Type: application/json (Status: {r.status_code}). Response: {r.text[:500]}")
            return None # Treat as a failed fetch
    else:
        # Handle cases where status is 200 but content is not JSON (e.g., empty string, HTML error page)
        print(f"Warning: OpenAlex API for {url} returned non-JSON content (Status: {r.status_code}, Content-Type: {r.headers.get('Content-Type')}). Response: {r.text[:500]}")
        return None # Treat as a failed fetch

    time.sleep(SLEEP_OPENALEX)
    if cache_key:
        save_cache(cache_key, data)
    return data

def normalize(s: str) -> str:
    return " ".join((s or "").lower().strip().split())

# --------------------------
# Load the data produced by the earlier pipeline
# --------------------------
paths_needed = {
    "country": os.path.join(OUTDIR, "citations_by_country.csv"),
    "inst": os.path.join(OUTDIR, "citations_by_institution_company.csv"),
    "edges": os.path.join(OUTDIR, "citation_edges_full.csv"),
    "enriched_master": os.path.join(OUTDIR, "citing_papers_master_enriched_openalex.csv"),
    "your_papers": os.path.join(OUTDIR, "your_papers.csv"),
    "top_researchers": os.path.join(OUTDIR, "top_citing_researchers.csv")
}

for k,p in paths_needed.items():
    if not os.path.exists(p):
        raise RuntimeError(f"Missing file: {p}\nRun the main pipeline cell first (the big one that created OUTDIR).")

df_country = pd.read_csv(paths_needed["country"])
df_inst    = pd.read_csv(paths_needed["inst"])
df_edges   = pd.read_csv(paths_needed["edges"])
df_master  = pd.read_csv(paths_needed["enriched_master"])
df_your    = pd.read_csv(paths_needed["your_papers"])
df_topr    = pd.read_csv(paths_needed["top_researchers"])

# Ensure expected columns exist
for col in ["country_code","citing_papers"]:
    if col not in df_country.columns:
        raise RuntimeError("citations_by_country.csv is missing expected columns.")

# ------------------------------------------------------------
# Helper: get institution geo (lat/lon) using OpenAlex search
# ------------------------------------------------------------
def openalex_match_institution_geo(inst_name: str):
    """
    Best-effort: searches OpenAlex institutions by name and returns geo
    {latitude, longitude, city, region, country_code} if available.
    """
    inst_name = (inst_name or "").strip()
    if not inst_name:
        return None

    key = f"inst_{hashlib.sha1(inst_name.lower().encode()).hexdigest()[:16]}.json"
    cached = load_cache(key)
    if cached is not None:
        return cached

    data = openalex_get(OPENALEX_INSTITUTIONS, params={"search": inst_name, "per_page": 5}, cache_key=None)
    cand = data.get("results", []) or []

    best = None
    best_score = -1
    for c in cand:
        name = c.get("display_name","") or ""
        score = fuzz.token_set_ratio(inst_name.lower(), name.lower())
        if score > best_score:
            best_score = score
            best = c

    if best is None or best_score < 70:
        save_cache(key, None)
        return None

    geo = best.get("geo") or {}
    out = {
        "openalex_institution_id": best.get("id",""),
        "matched_name": best.get("display_name",""),
        "match_score": best_score,
        "latitude": geo.get("latitude", None),
        "longitude": geo.get("longitude", None),
        "city": geo.get("city",""),
        "region": geo.get("region",""),
        "country_code": geo.get("country_code",""),
        "country": geo.get("country","")
    }
    save_cache(key, out)
    return out

# ------------------------------------------------------------
# Helper: best-effort author location via OpenAlex author search
# ------------------------------------------------------------
def openalex_match_author_location(author_name: str):
    """
    Best-effort: search OpenAlex authors and use last_known_institution geo.
    Returns dict with author + institution geo if available.
    """
    author_name = (author_name or "").strip()
    if not author_name:
        return None

    key = f"author_{hashlib.sha1(author_name.lower().encode()).hexdigest()[:16]}.json"
    cached = load_cache(key)
    if cached is not None:
        return cached

    data = openalex_get(OPENALEX_AUTHORS, params={"search": author_name, "per_page": 5}, cache_key=None)
    cand = data.get("results", []) or []

    best = None
    best_score = -1
    for c in cand:
        name = c.get("display_name","") or ""
        score = fuzz.token_set_ratio(author_name.lower(), name.lower())
        if score > best_score:
            best_score = score
            best = c

    if best is None or best_score < 75:
        save_cache(key, None)
        return None

    # last known institution may not include geo; use institution search by its name
    lki = best.get("last_known_institution") or {}
    inst_name = lki.get("display_name","") or ""
    inst_geo = openalex_match_institution_geo(inst_name) if inst_name else None

    out = {
        "author_query": author_name,
        "author_matched": best.get("display_name",""),
        "author_match_score": best_score,
        "openalex_author_id": best.get("id",""),
        "last_known_institution": inst_name,
        "inst_geo": inst_geo
    }
    save_cache(key, out)
    return out

# ============================================================
# 1) WORLD MAP: citations by country (choropleth)
# ============================================================
df_country_plot = df_country.copy()
df_country_plot["citing_papers"] = pd.to_numeric(df_country_plot["citing_papers"], errors="coerce").fillna(0).astype(int)

fig1 = px.choropleth(
    df_country_plot,
    locations="country_code",
    color="citing_papers",
    hover_name="country",
    title="World Map: Citing Papers by Country (Scholar-exact citing set; country from OpenAlex enrichment)"
)
map1_html = os.path.join(MAPDIR, "map_citations_by_country.html")
fig1.write_html(map1_html)

# ============================================================
# 2) WORLD MAP: citations by institution/company using lat/lon
# ============================================================
# Take top institutions to geocode (avoid too many queries)
TOP_INST_TO_GEOCODE = 300
df_inst_plot = df_inst.copy()
df_inst_plot["citing_papers"] = pd.to_numeric(df_inst_plot["citing_papers"], errors="coerce").fillna(0).astype(int)
df_inst_plot = df_inst_plot.sort_values("citing_papers", ascending=False).head(TOP_INST_TO_GEOCODE)

geo_rows = []
for _, r in df_inst_plot.iterrows():
    inst = r.get("institution_or_company") or r.get("institution") or r.get("institution_or_company", "")
    cc   = r.get("country_code","")
    cnt  = int(r.get("citing_papers", 0))
    g = openalex_match_institution_geo(inst)
    if g and g.get("latitude") is not None and g.get("longitude") is not None:
        geo_rows.append({
            "institution": inst,
            "matched_institution": g.get("matched_name",""),
            "match_score": g.get("match_score",0),
            "latitude": g.get("latitude"),
            "longitude": g.get("longitude", None),
            "country_code": g.get("country_code","") or cc,
            "country": g.get("country",""),
            "city": g.get("city",""),
            "region": g.get("region",""),
            "citing_papers": cnt
        })

df_inst_geo = pd.DataFrame(geo_rows).sort_values("citing_papers", ascending=False)
df_inst_geo.to_csv(os.path.join(MAPDIR, "institutions_with_geo.csv"), index=False)

fig2 = px.scatter_geo(
    df_inst_geo,
    lat="latitude",
    lon="longitude",
    size="citing_papers",
    hover_name="matched_institution",
    hover_data=["institution","country","city","region","citing_papers","match_score"],
    title="World Map: Citing Papers by Institution Location (lat/lon from OpenAlex institution geo; best-effort matching)"
)
map2_html = os.path.join(MAPDIR, "map_citations_by_institution_geo.html")
fig2.write_html(map2_html)

# ============================================================
# 3) WORLD MAP: top 5 papers (paper-wise) citations by country
# ============================================================
# Join edges to enriched master for country lists
df_master_small = df_master[["citing_key","openalex_countries"]].copy()
df_join = df_edges.merge(df_master_small, on="citing_key", how="left")

def parse_country_list(x):
    try:
        if isinstance(x, str) and x.strip():
            return json.loads(x)
    except Exception:
        pass
    return []

df_join["country_list"] = df_join["openalex_countries"].apply(parse_country_list)

# explode countries per edge
rows = []
for _, r in df_join.iterrows():
    yp = r.get("your_paper_title","")
    for cc in r.get("country_list", []):
        if cc:
            rows.append({"your_paper_title": yp, "country_code": cc})

df_paper_country = pd.DataFrame(rows)

# pick top 5 YOUR papers by Scholar citation count (df_your has your_paper_citations)
df_your["your_paper_citations"] = pd.to_numeric(df_your["your_paper_citations"], errors="coerce").fillna(0).astype(int)
top5_titles = df_your.sort_values("your_paper_citations", ascending=False)["your_paper_title"].head(5).tolist()

paper_maps = []
for t in top5_titles:
    sub = df_paper_country[df_paper_country["your_paper_title"] == t]
    if sub.empty:
        continue
    cnt = sub.groupby("country_code").size().reset_index(name="citing_papers")
    cnt = cnt.sort_values("citing_papers", ascending=False)

    # keep all for map; also save top5 table
    top5 = cnt.head(5).copy()
    top5_path = os.path.join(MAPDIR, f"top5_countries_for_paper__"+hashlib.sha1(t.encode()).hexdigest()[:10]+".csv")
    top5.to_csv(top5_path, index=False)

    fig = px.choropleth(
        cnt,
        locations="country_code",
        color="citing_papers",
        title=f"World Map: Citing Papers by Country (Paper-wise) — {t[:90]}"
    )
    html = os.path.join(MAPDIR, f"map_paperwise_country__"+hashlib.sha1(t.encode()).hexdigest()[:10]+".html")
    fig.write_html(html)
    paper_maps.append(html)

# Also write a simple index file listing these paper maps
with open(os.path.join(MAPDIR, "paperwise_top5_maps_index.txt"), "w", encoding="utf-8") as f:
    f.write("Paper-wise maps (top 5 by your citations):\n")
    for t in top5_titles:
        f.write(f"- {t}\n")

# ============================================================
# 4) WORLD MAP: top researchers locations for citations
# ============================================================
# TOP_RESEARCHERS_TO_LOCATE = 150
# df_topr2 = df_topr.copy()
# df_topr2["citing_papers"] = pd.to_numeric(df_topr2["citing_papers"], errors="coerce").fillna(0).astype(int)
# df_topr2 = df_topr2.sort_values("citing_papers", ascending=False).head(TOP_RESEARCHERS_TO_LOCATE)

# loc_rows = []
# for _, r in df_topr2.iterrows():
#     name = r.get("researcher","")
#     cnt  = int(r.get("citing_papers",0))
#     info = openalex_match_author_location(name)
#     if not info or not info.get("inst_geo"):
#         continue
#     g = info["inst_geo"]
#     if g and g.get("latitude") is not None and g.get("longitude") is not None:
#         loc_rows.append({
#             "researcher": info.get("author_matched", name),
#             "query_name": name,
#             "researcher_match_score": info.get("author_match_score", 0),
#             "institution": info.get("last_known_institution",""),
#             "latitude": g.get("latitude"),
#             "longitude": g.get("longitude", None),
#             "country_code": g.get("country_code",""),
#             "country": g.get("country",""),
#             "city": g.get("city",""),
#             "region": g.get("region",""),
#             "citing_papers": cnt
#         })

# df_researcher_geo = pd.DataFrame(loc_rows).sort_values("citing_papers", ascending=False)
# df_researcher_geo.to_csv(os.path.join(MAPDIR, "top_researchers_with_geo.csv"), index=False)

# fig4 = px.scatter_geo(
#     df_researcher_geo,
#     lat="latitude",
#     lon="longitude",
#     size="citing_papers",
#     hover_name="researcher",
#     hover_data=["institution","country","city","region","citing_papers","researcher_match_score"],
#     title="World Map: Top Citing Researchers Locations (best-effort via OpenAlex author last_known_institution geo)"
# )
# map4_html = os.path.join(MAPDIR, "map_top_researchers_locations.html")
# fig4.write_html(map4_html)

# ============================================================
# EXTRA USEFUL GRAPHS
# A) Top countries bar chart
# B) Top institutions bar chart
# C) Citation network graph (HTML)
# ============================================================
# A) Top countries bar
df_country_top = df_country_plot.sort_values("citing_papers", ascending=False).head(25)
figA = px.bar(df_country_top, x="country", y="citing_papers", title="Top 25 Countries (count of citing papers)")
figA.write_html(os.path.join(MAPDIR, "bar_top_countries.html"))

# B) Top institutions bar
df_inst_bar = df_inst.sort_values("citing_papers", ascending=False).head(25).copy()
label_col = "institution_or_company" if "institution_or_company" in df_inst_bar.columns else df_inst_bar.columns[0]
figB = px.bar(df_inst_bar, x=label_col, y="citing_papers", title="Top 25 Institutions/Companies (count of citing papers)")
figB.update_layout(xaxis_tickangle=-60)
figB.write_html(os.path.join(MAPDIR, "bar_top_institutions.html"))

# C) Citation network (sample if huge)
# Build graph from df_edges and export as interactive plotly scatter (spring layout)
MAX_EDGES_FOR_NETWORK = 2500
edges_small = df_edges.sample(n=min(MAX_EDGES_FOR_NETWORK, len(df_edges)), random_state=7) if len(df_edges) > MAX_EDGES_FOR_NETWORK else df_edges

G = nx.Graph()
for _, e in edges_small.iterrows():
    G.add_edge(e["citing_title"], e["your_paper_title"])

pos = nx.spring_layout(G, k=0.25, iterations=50, seed=7)
node_x, node_y, node_text, node_size = [], [], [], []
deg = dict(G.degree())

for n,(x,y) in pos.items():
    node_x.append(x)
    node_y.append(y)
    node_text.append(n)
    node_size.append(max(5, min(25, deg.get(n,1))))

edge_x, edge_y = [], []
for u,v in G.edges():
    x0,y0 = pos[u]
    x1,y1 = pos[v]
    edge_x += [x0, x1, None]
    edge_y += [y0, y1, None]

figC = go.Figure()
figC.add_trace(go.Scatter(x=edge_x, y=edge_y, mode="lines", line=dict(width=0.5), hoverinfo="none"))
figC.add_trace(go.Scatter(
    x=node_x, y=node_y, mode="markers", marker=dict(size=node_size),
    text=node_text, hoverinfo="text"
))
figC.update_layout(
    title="Citation Network (sampled edges if large)",
    showlegend=False,
    xaxis=dict(visible=False),
    yaxis=dict(visible=False),
    margin=dict(l=10,r=10,t=50,b=10)
)
figC.write_html(os.path.join(MAPDIR, "network_citation_map.html"))

# ============================================================
# Save a quick README describing outputs
# ============================================================
readme = f"""Maps generated in: {MAPDIR}

1) map_citations_by_country.html
   - choropleth by country_code using citations_by_country.csv

2) map_citations_by_institution_geo.html
   - institution/company points with lat/lon (best-effort OpenAlex institution geo match)
   - institutions_with_geo.csv contains matched lat/lon

3) Paper-wise maps (top 5 of YOUR papers by your Scholar citation counts):
   - map_paperwise_country__*.html (one per paper)
   - top5_countries_for_paper__*.csv (top 5 countries per paper)

4) map_top_researchers_locations.html
   - top researchers mapped using OpenAlex author last_known_institution geo (best-effort)
   - top_researchers_with_geo.csv contains matched lat/lon

Extras:
- bar_top_countries.html
- bar_top_institutions.html
- network_citation_map.html

Notes:
- Country/institution/researcher location layers depend on OpenAlex enrichment.
- Some institutions/authors may not resolve to geo => omitted from geo maps.
"""
with open(os.path.join(MAPDIR, "README_maps.txt"), "w", encoding="utf-8") as f:
    f.write(readme)

# ============================================================
# ZIP AND DOWNLOAD ALL MAPS
# ============================================================
zip_path = os.path.join(OUTDIR, "maps_bundle.zip")
with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as z:
    for root, _, fns in os.walk(MAPDIR):
        for fn in fns:
            full = os.path.join(root, fn)
            rel  = os.path.relpath(full, MAPDIR)
            z.write(full, arcname=rel)

print("\n✅ Maps created:")
print(" -", map1_html)
print(" -", map2_html)
# print(" -", map4_html)
print(" - plus paper-wise maps:", len(paper_maps))
print("✅ Zipped:", zip_path)

files.download(zip_path)

# ============================
# FIXED COUNTRY MAPS (ISO2->ISO3)
# Drop-in replacement for map #1 + map #3
# ============================

import os, json, hashlib
import pandas as pd
import pycountry
import plotly.express as px

OUTDIR = "scholar_rfe_outputs"  # must match your earlier pipeline
MAPDIR = os.path.join(OUTDIR, "maps")
os.makedirs(MAPDIR, exist_ok=True)

df_country = pd.read_csv(os.path.join(OUTDIR, "citations_by_country.csv"))

def iso2_to_iso3(code2: str) -> str:
    if not isinstance(code2, str):
        return ""
    c = code2.strip().upper()
    if not c:
        return ""
    # Common edge cases / aliases
    aliases = {
        "UK": "GB",
        "EL": "GR",
    }
    c = aliases.get(c, c)
    try:
        obj = pycountry.countries.get(alpha_2=c)
        return obj.alpha_3 if obj else ""
    except Exception:
        return ""

# ---- Build plot-ready country table ----
df_country_plot = df_country.copy()

# Ensure numeric
df_country_plot["citing_papers"] = pd.to_numeric(df_country_plot.get("citing_papers", 0), errors="coerce").fillna(0).astype(int)

# Convert ISO2 -> ISO3
if "country_code" not in df_country_plot.columns:
    raise RuntimeError("citations_by_country.csv must contain a 'country_code' column (ISO-2 codes).")

df_country_plot["iso3"] = df_country_plot["country_code"].apply(iso2_to_iso3)

# Filter invalid
df_country_plot = df_country_plot[(df_country_plot["iso3"] != "") & (df_country_plot["citing_papers"] > 0)].copy()

# Debug export so you can verify codes
debug_path = os.path.join(MAPDIR, "DEBUG_country_plot_input.csv")
df_country_plot.to_csv(debug_path, index=False)

print("Country rows used for map:", len(df_country_plot))
print("Saved debug input:", debug_path)
if len(df_country_plot) == 0:
    raise RuntimeError("No valid country rows after ISO2->ISO3 conversion. Check citations_by_country.csv content.")

# ---- Map #1: citations by country ----
fig1 = px.choropleth(
    df_country_plot,
    locations="iso3",
    locationmode="ISO-3",
    color="citing_papers",
    hover_name="country" if "country" in df_country_plot.columns else "iso3",
    hover_data={"iso3": True, "citing_papers": True},
    title="World Map: Citing Papers by Country (ISO-3 fixed)",
)
map1_html = os.path.join(MAPDIR, "map_citations_by_country_FIXED.html")
fig1.write_html(map1_html)
print("✅ Wrote:", map1_html)

# ============================
# Fix paper-wise country maps too (top 5 papers)
# ============================
df_edges   = pd.read_csv(os.path.join(OUTDIR, "citation_edges_full.csv"))
df_master  = pd.read_csv(os.path.join(OUTDIR, "citing_papers_master_enriched_openalex.csv"))
df_your    = pd.read_csv(os.path.join(OUTDIR, "your_papers.csv"))

# join edges->master to get openalex_countries list
df_join = df_edges.merge(df_master[["citing_key","openalex_countries"]], on="citing_key", how="left")

def parse_country_list(x):
    try:
        if isinstance(x, str) and x.strip():
            return json.loads(x)
    except Exception:
        pass
    return []

df_join["country_list"] = df_join["openalex_countries"].apply(parse_country_list)

# explode into per-paper per-country counts
rows = []
for _, r in df_join.iterrows():
    yp = r.get("your_paper_title","")
    for cc2 in r.get("country_list", []):
        if isinstance(cc2, str) and cc2.strip():
            rows.append({"your_paper_title": yp, "country_code": cc2.strip().upper()})

df_paper_country = pd.DataFrame(rows)

# choose top 5 of your papers by Scholar citations
df_your["your_paper_citations"] = pd.to_numeric(df_your["your_paper_citations"], errors="coerce").fillna(0).astype(int)
top5_titles = df_your.sort_values("your_paper_citations", ascending=False)["your_paper_title"].head(5).tolist()

paper_map_index = []
for t in top5_titles:
    sub = df_paper_country[df_paper_country["your_paper_title"] == t]
    if sub.empty:
        continue
    cnt = sub.groupby("country_code").size().reset_index(name="citing_papers")
    cnt["iso3"] = cnt["country_code"].apply(iso2_to_iso3)
    cnt = cnt[(cnt["iso3"] != "") & (cnt["citing_papers"] > 0)].copy()
    cnt = cnt.sort_values("citing_papers", ascending=False)

    # save top5 countries table
    top5 = cnt.head(5)[["country_code","iso3","citing_papers"]].copy()
    top5_path = os.path.join(MAPDIR, f"top5_countries_for_paper__{hashlib.sha1(t.encode()).hexdigest()[:10]}.csv")
    top5.to_csv(top5_path, index=False)

    # paper-wise map
    fig = px.choropleth(
        cnt,
        locations="iso3",
        locationmode="ISO-3",
        color="citing_papers",
        hover_data={"country_code": True, "iso3": True, "citing_papers": True},
        title=f"World Map: Citing Papers by Country (Paper-wise) — {t[:90]}",
    )
    html = os.path.join(MAPDIR, f"map_paperwise_country_FIXED__{hashlib.sha1(t.encode()).hexdigest()[:10]}.html")
    fig.write_html(html)
    paper_map_index.append((t, html, top5_path))

# write index
with open(os.path.join(MAPDIR, "paperwise_country_maps_FIXED_index.txt"), "w", encoding="utf-8") as f:
    f.write("Paper-wise country maps (ISO-3 fixed):\n\n")
    for t, html, top5csv in paper_map_index:
        f.write(f"- {t}\n  map: {os.path.basename(html)}\n  top5: {os.path.basename(top5csv)}\n\n")

print(f"✅ Wrote {len(paper_map_index)} paper-wise fixed maps in {MAPDIR}")

# ============================================================
# FIXED MAP #4: Top researchers locations (ID-based, accurate)
# - Uses OpenAlex Work -> authorships -> author_id + institution_id
# - Uses institution geo from OpenAlex institutions/{id}
# - Falls back to author last_known_institution only if needed
# ============================================================

!pip -q install requests plotly pandas

import os, json, time, hashlib
import pandas as pd
import requests
import plotly.express as px

# --------------------------
# CONFIG (match your pipeline)
# --------------------------
OUTDIR   = "scholar_rfe_outputs"
MAPDIR   = os.path.join(OUTDIR, "maps")
CACHEDIR = os.path.join(OUTDIR, "cache_geo")
os.makedirs(MAPDIR, exist_ok=True)
os.makedirs(CACHEDIR, exist_ok=True)

MAILTO = "nppatel7@asu.edu"   # keep same as your earlier code
SLEEP_OPENALEX = 0.25

OPENALEX_WORKS        = "https://api.openalex.org/works"
OPENALEX_AUTHORS      = "https://api.openalex.org/authors"
OPENALEX_INSTITUTIONS = "https://api.openalex.org/institutions"

sess = requests.Session()

def _cache_path(name: str) -> str:
    return os.path.join(CACHEDIR, name)

def load_cache(name: str):
    p = _cache_path(name)
    if os.path.exists(p):
        with open(p, "r", encoding="utf-8") as f:
            return json.load(f)
    return None

def save_cache(name: str, obj):
    p = _cache_path(name)
    with open(p, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False)

def openalex_get(url: str, params: dict = None, cache_key: str = None):
    if cache_key:
        cached = load_cache(cache_key)
        if cached is not None:
            return cached

    call_params = dict(params or {})
    call_params["mailto"] = MAILTO

    r = sess.get(url, params=call_params, timeout=120)
    r.raise_for_status()

    # OpenAlex is JSON
    data = r.json()
    time.sleep(SLEEP_OPENALEX)

    if cache_key:
        save_cache(cache_key, data)
    return data

def get_work_by_id(work_id_or_url: str):
    """
    Accepts:
      - https://openalex.org/Wxxxx
      - Wxxxx
    """
    wid = (work_id_or_url or "").strip()
    if not wid:
        return None
    if wid.startswith("https://openalex.org/"):
        wid = wid.split("/")[-1]
    key = f"work_{wid}.json"
    return openalex_get(f"{OPENALEX_WORKS}/{wid}", cache_key=key)

def get_author_by_id(author_id_or_url: str):
    aid = (author_id_or_url or "").strip()
    if not aid:
        return None
    if aid.startswith("https://openalex.org/"):
        aid = aid.split("/")[-1]
    key = f"author_{aid}.json"
    return openalex_get(f"{OPENALEX_AUTHORS}/{aid}", cache_key=key)

def get_inst_by_id(inst_id_or_url: str):
    iid = (inst_id_or_url or "").strip()
    if not iid:
        return None
    if iid.startswith("https://openalex.org/"):
        iid = iid.split("/")[-1]
    key = f"inst_{iid}.json"
    return openalex_get(f"{OPENALEX_INSTITUTIONS}/{iid}", cache_key=key)

def inst_geo_from_inst_obj(inst_obj):
    if not inst_obj:
        return None
    geo = inst_obj.get("geo") or {}
    lat = geo.get("latitude", None)
    lon = geo.get("longitude", None)
    if lat is None or lon is None:
        return None
    return {
        "institution_id": inst_obj.get("id",""),
        "institution_name": inst_obj.get("display_name",""),
        "latitude": lat,
        "longitude": lon,
        "city": geo.get("city",""),
        "region": geo.get("region",""),
        "country_code": geo.get("country_code",""),
        "country": geo.get("country",""),
    }

# --------------------------
# Load required pipeline outputs
# --------------------------
paths = {
    "edges":  os.path.join(OUTDIR, "citation_edges_full.csv"),
    "master": os.path.join(OUTDIR, "citing_papers_master_enriched_openalex.csv"),
}
for k,p in paths.items():
    if not os.path.exists(p):
        raise RuntimeError(f"Missing file: {p} (run the main pipeline first)")

df_edges  = pd.read_csv(paths["edges"])
df_master = pd.read_csv(paths["master"])

# Need citing_key + openalex_work_id
need_cols = {"citing_key","openalex_work_id"}
missing = [c for c in need_cols if c not in df_master.columns]
if missing:
    raise RuntimeError(f"citing_papers_master_enriched_openalex.csv missing columns: {missing}")

# --------------------------
# Build per-citing-paper weight:
# If a citing paper cites multiple of your papers, edges count >1
# We'll compute:
#   edges_per_citing_key = number of citation edges contributed by that citing paper
# --------------------------
edges_per_key = df_edges.groupby("citing_key").size().to_dict()

# We'll also keep a small example of which of your papers they cited
your_papers_by_key = df_edges.groupby("citing_key")["your_paper_title"].apply(lambda s: sorted(set(s))).to_dict()

# --------------------------
# Extract authorship institutions (accurate)
# --------------------------
author_stats = {}  # author_id -> dict stats

def add_author_hit(author_id, author_name, orcid, inst_id, inst_name, citing_key, work_id, citing_title, edge_weight):
    st = author_stats.get(author_id)
    if st is None:
        st = {
            "author_id": author_id,
            "author_name": author_name,
            "orcid": orcid or "",
            "citing_papers_count": 0,
            "citation_edges_count": 0,
            "sample_citing_key": citing_key,
            "sample_work_id": work_id,
            "sample_citing_title": (citing_title or "")[:250],
            "inst_id": inst_id or "",
            "inst_name": inst_name or "",
            "your_papers_cited_example": "; ".join(your_papers_by_key.get(citing_key, [])[:6]),
        }
        author_stats[author_id] = st

    # count unique citing papers
    st["citing_papers_count"] += 1
    st["citation_edges_count"] += int(edge_weight)

# Iterate all citing papers we have OpenAlex IDs for
rows_master = df_master[["citing_key","openalex_work_id","citing_title"]].dropna(subset=["openalex_work_id"]).copy()

print("Citing papers with OpenAlex work IDs:", len(rows_master))

for _, r in rows_master.iterrows():
    citing_key   = r["citing_key"]
    work_id      = r["openalex_work_id"]
    citing_title = r.get("citing_title","")

    edge_weight = edges_per_key.get(citing_key, 1)

    work = get_work_by_id(work_id)
    if not work:
        continue

    authorships = work.get("authorships") or []
    for a in authorships:
        author = a.get("author") or {}
        author_id = author.get("id","")
        author_name = author.get("display_name","") or ""

        ids = author.get("orcid") or ""  # sometimes in author object
        # If missing here, we can fetch later from author endpoint

        # Institutions for THIS paper (best signal)
        insts = a.get("institutions") or []
        inst_id, inst_name = "", ""
        if insts:
            inst_id = insts[0].get("id","") or ""
            inst_name = insts[0].get("display_name","") or ""

        if author_id:
            add_author_hit(author_id, author_name, ids, inst_id, inst_name, citing_key, work_id, citing_title, edge_weight)

# --------------------------
# Turn into dataframe + pick top N
# --------------------------
df_auth = pd.DataFrame(list(author_stats.values()))
if df_auth.empty:
    raise RuntimeError("No authors extracted from OpenAlex works. Check openalex_work_id coverage.")

df_auth["citing_papers_count"] = pd.to_numeric(df_auth["citing_papers_count"], errors="coerce").fillna(0).astype(int)
df_auth["citation_edges_count"] = pd.to_numeric(df_auth["citation_edges_count"], errors="coerce").fillna(0).astype(int)

TOP_RESEARCHERS_TO_MAP = 200
df_top = df_auth.sort_values(["citation_edges_count","citing_papers_count"], ascending=False).head(TOP_RESEARCHERS_TO_MAP).copy()

# --------------------------
# Resolve geo: institution_id first (paper institution), fallback to author last_known_institution
# --------------------------
mapped_rows = []
unmapped = []

for _, r in df_top.iterrows():
    author_id = r["author_id"]
    author_name = r["author_name"]

    geo = None
    geo_source = ""
    inst_id = r.get("inst_id","")
    inst_name = r.get("inst_name","")

    # (A) Try paper institution geo
    if inst_id:
        inst_obj = get_inst_by_id(inst_id)
        geo = inst_geo_from_inst_obj(inst_obj)
        if geo:
            geo_source = "authorship_institution"

    # (B) Fallback: author last_known_institution geo
    if geo is None:
        aobj = get_author_by_id(author_id)
        lki = (aobj or {}).get("last_known_institution") or {}
        lki_id = lki.get("id","") or ""
        lki_name = lki.get("display_name","") or ""
        if lki_id:
            inst_obj2 = get_inst_by_id(lki_id)
            geo = inst_geo_from_inst_obj(inst_obj2)
            if geo:
                geo_source = "author_last_known_institution"
                # replace inst fields to reflect what was used
                inst_id, inst_name = lki_id, lki_name

        # also patch ORCID if we can
        if aobj and (not r.get("orcid")):
            r["orcid"] = aobj.get("orcid") or r.get("orcid","")

    if geo is None:
        unmapped.append({
            "author_id": author_id,
            "author_name": author_name,
            "inst_id_seen": inst_id,
            "inst_name_seen": inst_name,
            "reason": "no_geo_on_inst_or_last_known_inst"
        })
        continue

    mapped_rows.append({
        "author_id": author_id,
        "author_name": author_name,
        "orcid": r.get("orcid",""),
        "citation_edges_count": int(r["citation_edges_count"]),
        "citing_papers_count": int(r["citing_papers_count"]),
        "institution_id": inst_id,
        "institution_name": inst_name,
        "geo_source": geo_source,
        "latitude": geo["latitude"],
        "longitude": geo["longitude"],
        "city": geo.get("city",""),
        "region": geo.get("region",""),
        "country_code": geo.get("country_code",""),
        "country": geo.get("country",""),
        "sample_citing_title": r.get("sample_citing_title",""),
        "your_papers_cited_example": r.get("your_papers_cited_example",""),
        "sample_work_id": r.get("sample_work_id",""),
    })

df_geo = pd.DataFrame(mapped_rows).sort_values(["citation_edges_count","citing_papers_count"], ascending=False)
df_geo_path = os.path.join(MAPDIR, "top_researchers_with_geo.csv")
df_geo.to_csv(df_geo_path, index=False)

df_unmapped = pd.DataFrame(unmapped)
unmapped_path = os.path.join(MAPDIR, "top_researchers_debug_unmapped.csv")
df_unmapped.to_csv(unmapped_path, index=False)

print("Mapped researchers:", len(df_geo), "| Unmapped:", len(df_unmapped))
print("Saved:", df_geo_path)
print("Saved:", unmapped_path)

# --------------------------
# Map: researcher locations
# --------------------------
fig = px.scatter_geo(
    df_geo,
    lat="latitude",
    lon="longitude",
    size="citation_edges_count",
    hover_name="author_name",
    hover_data=[
        "institution_name","country","city","region",
        "citation_edges_count","citing_papers_count",
        "geo_source","orcid"
    ],
    title="World Map: Top Citing Researchers Locations (ID-based via OpenAlex authorships + institution geo)"
)
map_html = os.path.join(MAPDIR, "map_top_researchers_locations.html")
fig.write_html(map_html)
print("✅ Wrote:", map_html)

# --------------------------
# Extra: Top researchers bar
# --------------------------
top_bar = df_geo.head(30).copy()
figb = px.bar(
    top_bar.sort_values("citation_edges_count", ascending=True),
    x="citation_edges_count",
    y="author_name",
    orientation="h",
    title="Top 30 Citing Researchers (by citation edges count)",
    hover_data=["institution_name","country","citing_papers_count","geo_source"]
)
bar_html = os.path.join(MAPDIR, "bar_top_researchers.html")
figb.write_html(bar_html)
print("✅ Wrote:", bar_html)

# Make institute bubbles bigger (drop-in replacement for your fig2 block)

import numpy as np
import plotly.express as px
import os

MAPDIR = "/content/scholar_rfe_outputs/maps"
inst_geo_csv = os.path.join(MAPDIR, "institutions_with_geo.csv")  # created by your code
df_inst_geo = pd.read_csv(inst_geo_csv)

# Ensure numeric
df_inst_geo["citing_papers"] = pd.to_numeric(df_inst_geo["citing_papers"], errors="coerce").fillna(0)

# Optional: boost visibility of small institutions
# sqrt keeps relative ranking but increases small bubbles
df_inst_geo["size_for_plot"] = np.sqrt(df_inst_geo["citing_papers"].clip(lower=1))

fig2 = px.scatter_geo(
    df_inst_geo,
    lat="latitude",
    lon="longitude",
    size="size_for_plot",          # <-- use boosted size
    size_max=12,                   # <-- increase bubble max size (try 30-60)
    hover_name="matched_institution",
    hover_data=["institution","country","city","region","citing_papers","match_score"],
    title="World Map: Citing Papers by Institution Location (bigger bubbles)"
)

# Optional: make bubble edges slightly clearer
fig2.update_traces(marker=dict(opacity=0.75, line=dict(width=0.5)))

map2_html = os.path.join(MAPDIR, "map_citations_by_institution_geo_BIG_BUBBLES.html")
fig2.write_html(map2_html)
print("✅ Wrote:", map2_html)

# ============================================
# Colab: Install missing system libs + Export ALL HTML -> PNG + PDF
# ============================================

# 1) System deps required by Chromium in Colab
!apt-get -qq update
!apt-get -qq install -y \
  libatk1.0-0 libatk-bridge2.0-0 libcups2 libxkbcommon0 libxcomposite1 libxdamage1 libxrandr2 \
  libgbm1 libpango-1.0-0 libpangocairo-1.0-0 libgtk-3-0 libnss3 libxss1 libasound2

# 2) Python deps + Chromium
!pip -q install playwright nest_asyncio pandas
!playwright install chromium

import os, re, zipfile, asyncio
import pandas as pd
import nest_asyncio
from google.colab import files
from playwright.async_api import async_playwright

nest_asyncio.apply()

ROOT = "/content/scholar_rfe_outputs"
OUT_IMG = os.path.join(ROOT, "images_export")
os.makedirs(OUT_IMG, exist_ok=True)

def sanitize_name(path: str) -> str:
    rel = os.path.relpath(path, ROOT).replace("\\", "/")
    rel = rel.replace(".html", "")
    rel = re.sub(r"[^A-Za-z0-9._/-]+", "_", rel)
    rel = rel.replace("/", "__")
    return rel

# Collect all HTML files
html_files = []
for root, _, fns in os.walk(ROOT):
    for fn in fns:
        if fn.lower().endswith(".html"):
            html_files.append(os.path.join(root, fn))

print(f"Found {len(html_files)} HTML files under {ROOT}")
if not html_files:
    raise RuntimeError("No .html files found. Check ROOT path.")

LAUNCH_ARGS = ["--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu"]

async def export_all():
    manifest_rows = []
    failed = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, args=LAUNCH_ARGS)
        context = await browser.new_context(
            viewport={"width": 2600, "height": 1500},
            device_scale_factor=3  # bump to 4 for even sharper, bigger files
        )

        for hp in sorted(html_files):
            base = os.path.join(OUT_IMG, sanitize_name(hp))
            out_png = base + ".png"
            out_pdf = base + ".pdf"

            try:
                page = await context.new_page()
                url = "file://" + hp

                # For local HTML, "load" is most reliable
                await page.goto(url, wait_until="load", timeout=60000)

                # Give Plotly time to render
                await page.wait_for_timeout(2000)

                # Best-effort: wait for plotly plot container if present
                try:
                    await page.wait_for_selector(".plotly, .js-plotly-plot", timeout=5000)
                except Exception:
                    pass

                # High-quality PNG
                await page.screenshot(path=out_png, full_page=True)

                # High-quality PDF (great for printing)
                await page.pdf(
                    path=out_pdf,
                    print_background=True,
                    landscape=True,
                    prefer_css_page_size=True
                )

                await page.close()
                manifest_rows.append({"html": hp, "png": out_png, "pdf": out_pdf})

            except Exception as e:
                failed.append({"html": hp, "error": str(e)})
                try:
                    await page.close()
                except Exception:
                    pass

        await context.close()
        await browser.close()

    # Write manifests
    manifest_path = os.path.join(OUT_IMG, "manifest.csv")
    pd.DataFrame(manifest_rows).to_csv(manifest_path, index=False)

    failed_path = os.path.join(OUT_IMG, "failed.csv")
    pd.DataFrame(failed).to_csv(failed_path, index=False)

    # Zip outputs
    zip_path = os.path.join(ROOT, "graphs_high_quality_images.zip")
    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as z:
        for r in manifest_rows:
            z.write(r["png"], arcname=os.path.relpath(r["png"], ROOT))
            z.write(r["pdf"], arcname=os.path.relpath(r["pdf"], ROOT))
        z.write(manifest_path, arcname=os.path.relpath(manifest_path, ROOT))
        z.write(failed_path, arcname=os.path.relpath(failed_path, ROOT))

    print(f"✅ Exported {len(manifest_rows)}/{len(html_files)} HTML files")
    if failed:
        print(f"⚠️ Failed {len(failed)} files. See: {failed_path}")
    print("✅ Zipped:", zip_path)

    files.download(zip_path)

await export_all()



import os
import zipfile
from google.colab import files

folder_to_zip = "/content/scholar_rfe_outputs"
zip_filename = "Nisarg_RFE_Data.zip"

# Create a ZipFile object in write mode
with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
    # Iterate over all the files and subfolders in the target directory
    for root, dirs, files_in_dir in os.walk(folder_to_zip):
        for file in files_in_dir:
            file_path = os.path.join(root, file)
            # Add file to zip archive, maintaining its relative path within the folder
            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))

print(f"Successfully created {zip_filename} from {folder_to_zip}")

# Download the created zip file
files.download(zip_filename)

